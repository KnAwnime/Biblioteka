
 /home/lsakka/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: "Sym(s1)", arg1_1: "i64[s1][1]cuda:0", arg2_1: "bf16[128256, 4096][4096, 1]cuda:0", arg3_1: "bf16[4096][1]cuda:0", arg4_1: "bf16[6144, 4096][4096, 1]cuda:0", arg5_1: "bf16[8192, 128][128, 1]cuda:0", arg6_1: "Sym(s4)", arg7_1: "i64[s4][1]cuda:0", arg8_1: "Sym(s5)", arg9_1: "Sym(s6)", arg10_1: "Sym(s7)", arg11_1: "Sym(s8)", arg12_1: "Sym(s9)", arg13_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg14_1: "Sym(s10)", arg15_1: "i64[s10][1]cuda:0", arg16_1: "Sym(s1)", arg17_1: "Sym(s14)", arg18_1: "Sym(s16)", arg19_1: "i32[s16][1]cuda:0", arg20_1: "bf16[4096, 4096][4096, 1]cuda:0", arg21_1: "bf16[4096][1]cuda:0", arg22_1: "bf16[28672, 4096][4096, 1]cuda:0", arg23_1: "bf16[4096, 14336][14336, 1]cuda:0", arg24_1: "bf16[4096][1]cuda:0", arg25_1: "bf16[6144, 4096][4096, 1]cuda:0", arg26_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg27_1: "bf16[4096, 4096][4096, 1]cuda:0", arg28_1: "bf16[4096][1]cuda:0", arg29_1: "bf16[28672, 4096][4096, 1]cuda:0", arg30_1: "bf16[4096, 14336][14336, 1]cuda:0", arg31_1: "bf16[4096][1]cuda:0", arg32_1: "bf16[6144, 4096][4096, 1]cuda:0", arg33_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg34_1: "bf16[4096, 4096][4096, 1]cuda:0", arg35_1: "bf16[4096][1]cuda:0", arg36_1: "bf16[28672, 4096][4096, 1]cuda:0", arg37_1: "bf16[4096, 14336][14336, 1]cuda:0", arg38_1: "bf16[4096][1]cuda:0", arg39_1: "bf16[6144, 4096][4096, 1]cuda:0", arg40_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg41_1: "bf16[4096, 4096][4096, 1]cuda:0", arg42_1: "bf16[4096][1]cuda:0", arg43_1: "bf16[28672, 4096][4096, 1]cuda:0", arg44_1: "bf16[4096, 14336][14336, 1]cuda:0", arg45_1: "bf16[4096][1]cuda:0", arg46_1: "bf16[6144, 4096][4096, 1]cuda:0", arg47_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg48_1: "bf16[4096, 4096][4096, 1]cuda:0", arg49_1: "bf16[4096][1]cuda:0", arg50_1: "bf16[28672, 4096][4096, 1]cuda:0", arg51_1: "bf16[4096, 14336][14336, 1]cuda:0", arg52_1: "bf16[4096][1]cuda:0", arg53_1: "bf16[6144, 4096][4096, 1]cuda:0", arg54_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg55_1: "bf16[4096, 4096][4096, 1]cuda:0", arg56_1: "bf16[4096][1]cuda:0", arg57_1: "bf16[28672, 4096][4096, 1]cuda:0", arg58_1: "bf16[4096, 14336][14336, 1]cuda:0", arg59_1: "bf16[4096][1]cuda:0", arg60_1: "bf16[6144, 4096][4096, 1]cuda:0", arg61_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg62_1: "bf16[4096, 4096][4096, 1]cuda:0", arg63_1: "bf16[4096][1]cuda:0", arg64_1: "bf16[28672, 4096][4096, 1]cuda:0", arg65_1: "bf16[4096, 14336][14336, 1]cuda:0", arg66_1: "bf16[4096][1]cuda:0", arg67_1: "bf16[6144, 4096][4096, 1]cuda:0", arg68_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg69_1: "bf16[4096, 4096][4096, 1]cuda:0", arg70_1: "bf16[4096][1]cuda:0", arg71_1: "bf16[28672, 4096][4096, 1]cuda:0", arg72_1: "bf16[4096, 14336][14336, 1]cuda:0", arg73_1: "bf16[4096][1]cuda:0", arg74_1: "bf16[6144, 4096][4096, 1]cuda:0", arg75_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg76_1: "bf16[4096, 4096][4096, 1]cuda:0", arg77_1: "bf16[4096][1]cuda:0", arg78_1: "bf16[28672, 4096][4096, 1]cuda:0", arg79_1: "bf16[4096, 14336][14336, 1]cuda:0", arg80_1: "bf16[4096][1]cuda:0", arg81_1: "bf16[6144, 4096][4096, 1]cuda:0", arg82_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg83_1: "bf16[4096, 4096][4096, 1]cuda:0", arg84_1: "bf16[4096][1]cuda:0", arg85_1: "bf16[28672, 4096][4096, 1]cuda:0", arg86_1: "bf16[4096, 14336][14336, 1]cuda:0", arg87_1: "bf16[4096][1]cuda:0", arg88_1: "bf16[6144, 4096][4096, 1]cuda:0", arg89_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg90_1: "bf16[4096, 4096][4096, 1]cuda:0", arg91_1: "bf16[4096][1]cuda:0", arg92_1: "bf16[28672, 4096][4096, 1]cuda:0", arg93_1: "bf16[4096, 14336][14336, 1]cuda:0", arg94_1: "bf16[4096][1]cuda:0", arg95_1: "bf16[6144, 4096][4096, 1]cuda:0", arg96_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg97_1: "bf16[4096, 4096][4096, 1]cuda:0", arg98_1: "bf16[4096][1]cuda:0", arg99_1: "bf16[28672, 4096][4096, 1]cuda:0", arg100_1: "bf16[4096, 14336][14336, 1]cuda:0", arg101_1: "bf16[4096][1]cuda:0", arg102_1: "bf16[6144, 4096][4096, 1]cuda:0", arg103_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg104_1: "bf16[4096, 4096][4096, 1]cuda:0", arg105_1: "bf16[4096][1]cuda:0", arg106_1: "bf16[28672, 4096][4096, 1]cuda:0", arg107_1: "bf16[4096, 14336][14336, 1]cuda:0", arg108_1: "bf16[4096][1]cuda:0", arg109_1: "bf16[6144, 4096][4096, 1]cuda:0", arg110_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg111_1: "bf16[4096, 4096][4096, 1]cuda:0", arg112_1: "bf16[4096][1]cuda:0", arg113_1: "bf16[28672, 4096][4096, 1]cuda:0", arg114_1: "bf16[4096, 14336][14336, 1]cuda:0", arg115_1: "bf16[4096][1]cuda:0", arg116_1: "bf16[6144, 4096][4096, 1]cuda:0", arg117_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg118_1: "bf16[4096, 4096][4096, 1]cuda:0", arg119_1: "bf16[4096][1]cuda:0", arg120_1: "bf16[28672, 4096][4096, 1]cuda:0", arg121_1: "bf16[4096, 14336][14336, 1]cuda:0", arg122_1: "bf16[4096][1]cuda:0", arg123_1: "bf16[6144, 4096][4096, 1]cuda:0", arg124_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg125_1: "bf16[4096, 4096][4096, 1]cuda:0", arg126_1: "bf16[4096][1]cuda:0", arg127_1: "bf16[28672, 4096][4096, 1]cuda:0", arg128_1: "bf16[4096, 14336][14336, 1]cuda:0", arg129_1: "bf16[4096][1]cuda:0", arg130_1: "bf16[6144, 4096][4096, 1]cuda:0", arg131_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg132_1: "bf16[4096, 4096][4096, 1]cuda:0", arg133_1: "bf16[4096][1]cuda:0", arg134_1: "bf16[28672, 4096][4096, 1]cuda:0", arg135_1: "bf16[4096, 14336][14336, 1]cuda:0", arg136_1: "bf16[4096][1]cuda:0", arg137_1: "bf16[6144, 4096][4096, 1]cuda:0", arg138_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg139_1: "bf16[4096, 4096][4096, 1]cuda:0", arg140_1: "bf16[4096][1]cuda:0", arg141_1: "bf16[28672, 4096][4096, 1]cuda:0", arg142_1: "bf16[4096, 14336][14336, 1]cuda:0", arg143_1: "bf16[4096][1]cuda:0", arg144_1: "bf16[6144, 4096][4096, 1]cuda:0", arg145_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg146_1: "bf16[4096, 4096][4096, 1]cuda:0", arg147_1: "bf16[4096][1]cuda:0", arg148_1: "bf16[28672, 4096][4096, 1]cuda:0", arg149_1: "bf16[4096, 14336][14336, 1]cuda:0", arg150_1: "bf16[4096][1]cuda:0", arg151_1: "bf16[6144, 4096][4096, 1]cuda:0", arg152_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg153_1: "bf16[4096, 4096][4096, 1]cuda:0", arg154_1: "bf16[4096][1]cuda:0", arg155_1: "bf16[28672, 4096][4096, 1]cuda:0", arg156_1: "bf16[4096, 14336][14336, 1]cuda:0", arg157_1: "bf16[4096][1]cuda:0", arg158_1: "bf16[6144, 4096][4096, 1]cuda:0", arg159_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg160_1: "bf16[4096, 4096][4096, 1]cuda:0", arg161_1: "bf16[4096][1]cuda:0", arg162_1: "bf16[28672, 4096][4096, 1]cuda:0", arg163_1: "bf16[4096, 14336][14336, 1]cuda:0", arg164_1: "bf16[4096][1]cuda:0", arg165_1: "bf16[6144, 4096][4096, 1]cuda:0", arg166_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg167_1: "bf16[4096, 4096][4096, 1]cuda:0", arg168_1: "bf16[4096][1]cuda:0", arg169_1: "bf16[28672, 4096][4096, 1]cuda:0", arg170_1: "bf16[4096, 14336][14336, 1]cuda:0", arg171_1: "bf16[4096][1]cuda:0", arg172_1: "bf16[6144, 4096][4096, 1]cuda:0", arg173_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg174_1: "bf16[4096, 4096][4096, 1]cuda:0", arg175_1: "bf16[4096][1]cuda:0", arg176_1: "bf16[28672, 4096][4096, 1]cuda:0", arg177_1: "bf16[4096, 14336][14336, 1]cuda:0", arg178_1: "bf16[4096][1]cuda:0", arg179_1: "bf16[6144, 4096][4096, 1]cuda:0", arg180_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg181_1: "bf16[4096, 4096][4096, 1]cuda:0", arg182_1: "bf16[4096][1]cuda:0", arg183_1: "bf16[28672, 4096][4096, 1]cuda:0", arg184_1: "bf16[4096, 14336][14336, 1]cuda:0", arg185_1: "bf16[4096][1]cuda:0", arg186_1: "bf16[6144, 4096][4096, 1]cuda:0", arg187_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg188_1: "bf16[4096, 4096][4096, 1]cuda:0", arg189_1: "bf16[4096][1]cuda:0", arg190_1: "bf16[28672, 4096][4096, 1]cuda:0", arg191_1: "bf16[4096, 14336][14336, 1]cuda:0", arg192_1: "bf16[4096][1]cuda:0", arg193_1: "bf16[6144, 4096][4096, 1]cuda:0", arg194_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg195_1: "bf16[4096, 4096][4096, 1]cuda:0", arg196_1: "bf16[4096][1]cuda:0", arg197_1: "bf16[28672, 4096][4096, 1]cuda:0", arg198_1: "bf16[4096, 14336][14336, 1]cuda:0", arg199_1: "bf16[4096][1]cuda:0", arg200_1: "bf16[6144, 4096][4096, 1]cuda:0", arg201_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg202_1: "bf16[4096, 4096][4096, 1]cuda:0", arg203_1: "bf16[4096][1]cuda:0", arg204_1: "bf16[28672, 4096][4096, 1]cuda:0", arg205_1: "bf16[4096, 14336][14336, 1]cuda:0", arg206_1: "bf16[4096][1]cuda:0", arg207_1: "bf16[6144, 4096][4096, 1]cuda:0", arg208_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg209_1: "bf16[4096, 4096][4096, 1]cuda:0", arg210_1: "bf16[4096][1]cuda:0", arg211_1: "bf16[28672, 4096][4096, 1]cuda:0", arg212_1: "bf16[4096, 14336][14336, 1]cuda:0", arg213_1: "bf16[4096][1]cuda:0", arg214_1: "bf16[6144, 4096][4096, 1]cuda:0", arg215_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg216_1: "bf16[4096, 4096][4096, 1]cuda:0", arg217_1: "bf16[4096][1]cuda:0", arg218_1: "bf16[28672, 4096][4096, 1]cuda:0", arg219_1: "bf16[4096, 14336][14336, 1]cuda:0", arg220_1: "bf16[4096][1]cuda:0", arg221_1: "bf16[6144, 4096][4096, 1]cuda:0", arg222_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg223_1: "bf16[4096, 4096][4096, 1]cuda:0", arg224_1: "bf16[4096][1]cuda:0", arg225_1: "bf16[28672, 4096][4096, 1]cuda:0", arg226_1: "bf16[4096, 14336][14336, 1]cuda:0", arg227_1: "bf16[4096][1]cuda:0", arg228_1: "bf16[6144, 4096][4096, 1]cuda:0", arg229_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg230_1: "bf16[4096, 4096][4096, 1]cuda:0", arg231_1: "bf16[4096][1]cuda:0", arg232_1: "bf16[28672, 4096][4096, 1]cuda:0", arg233_1: "bf16[4096, 14336][14336, 1]cuda:0", arg234_1: "bf16[4096][1]cuda:0", arg235_1: "bf16[6144, 4096][4096, 1]cuda:0", arg236_1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0", arg237_1: "bf16[4096, 4096][4096, 1]cuda:0", arg238_1: "bf16[4096][1]cuda:0", arg239_1: "bf16[28672, 4096][4096, 1]cuda:0", arg240_1: "bf16[4096, 14336][14336, 1]cuda:0", arg241_1: "bf16[4096][1]cuda:0"):
         # File: /home/lsakka/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py:43 in embedding, code: return F.embedding(input_, layer.weight)
        embedding: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.embedding.default(arg2_1, arg1_1);  arg2_1 = arg1_1 = None

        # No stacktrace found for following nodes
        tp_in_place_ar_default = torch.ops.vllm.tp_in_place_ar.default(embedding);  tp_in_place_ar_default = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/layernorm.py:61 in forward_cuda, code: out = torch.empty_like(x)
        empty: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg16_1, 4096], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(empty, [0, 1]);  empty = None

        # No stacktrace found for following nodes
        as_strided_default_160: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.as_strided.default(permute, [arg16_1, 4096], [4096, 1], 0)
        rms_norm_default = torch.ops._C.rms_norm.default(as_strided_default_160, embedding, arg3_1, 1e-05);  as_strided_default_160 = arg3_1 = rms_norm_default = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_1: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg4_1, [1, 0]);  arg4_1 = None
        mm: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(permute, permute_1);  permute = permute_1 = None

        # No stacktrace found for following nodes
        as_strided_default_158: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm, [arg16_1, 4096], [6144, 1], 0)
        as_strided_default_159: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm, [arg16_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_31 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_158, as_strided_default_159, 128, arg5_1, True);  as_strided_default_158 = as_strided_default_159 = rotary_embedding_default_31 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(mm, [4096, 1024, 1024], -1)
        getitem_10: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_1[1];  split_with_sizes_1 = None
        split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(mm, [4096, 1024, 1024], -1)
        getitem_14: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_2[2];  split_with_sizes_2 = None
        view_3: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_10, [-1, 8, 128]);  getitem_10 = None
        view_4: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_14, [-1, 8, 128]);  getitem_14 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg13_1, 1)
        sym_stride_int_1: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg13_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_1: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg13_1, 0, 1)
        sym_storage_offset_default: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_1);  select_1 = None

        # No stacktrace found for following nodes
        as_strided_default_156: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg13_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int, sym_stride_int_1, arg12_1, 1], 0)
        as_strided_default_157: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg13_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int, sym_stride_int_1, arg12_1, 1], sym_storage_offset_default);  sym_stride_int = sym_stride_int_1 = sym_storage_offset_default = None
        reshape_and_cache_flash_default_31 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_3, view_4, as_strided_default_156, as_strided_default_157, arg15_1, 'auto', 1.0, 1.0);  view_3 = view_4 = as_strided_default_156 = as_strided_default_157 = reshape_and_cache_flash_default_31 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_1: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg16_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_2: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_1, [0, 1, 2]);  empty_1 = permute_2 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(mm, [4096, 1024, 1024], -1)
        getitem_20: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_4[0];  split_with_sizes_4 = None
        split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(mm, [4096, 1024, 1024], -1)
        getitem_24: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_5[1];  split_with_sizes_5 = None
        split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(mm, [4096, 1024, 1024], -1);  mm = None
        getitem_28: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_6[2];  split_with_sizes_6 = None
        view_6: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_20, [-1, 32, 128]);  getitem_20 = None
        slice_4: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_6, 0, 0, arg16_1);  view_6 = None
        view_7: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_24, [-1, 8, 128]);  getitem_24 = None
        slice_5: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_7, 0, 0, arg16_1);  view_7 = None
        view_8: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_28, [-1, 8, 128]);  getitem_28 = None
        slice_6: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_8, 0, 0, arg16_1);  view_8 = None
        flash_attn_varlen_func: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_4, slice_5, slice_6, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_4 = slice_5 = slice_6 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_10: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func, [arg0_1, 4096]);  flash_attn_varlen_func = None
        permute_3: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg20_1, [1, 0]);  arg20_1 = None
        mm_1: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_3);  view_10 = permute_3 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_63 = torch.ops._C.fused_add_rms_norm.default(mm_1, embedding, arg21_1, 1e-05);  arg21_1 = fused_add_rms_norm_default_63 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_4: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg22_1, [1, 0]);  arg22_1 = None
        mm_2: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_1, permute_4);  mm_1 = permute_4 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_2: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_155: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_2, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_31 = torch.ops._C.silu_and_mul.default(as_strided_default_155, mm_2);  as_strided_default_155 = mm_2 = silu_and_mul_default_31 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_5: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg23_1, [1, 0]);  arg23_1 = None
        mm_3: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_2, permute_5);  empty_2 = permute_5 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_62 = torch.ops._C.fused_add_rms_norm.default(mm_3, embedding, arg24_1, 1e-05);  arg24_1 = fused_add_rms_norm_default_62 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_6: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg25_1, [1, 0]);  arg25_1 = None
        mm_4: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_3, permute_6);  mm_3 = permute_6 = None

        # No stacktrace found for following nodes
        as_strided_default_153: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_4, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_154: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_4, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_30 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_153, as_strided_default_154, 128, arg5_1, True);  as_strided_default_153 = as_strided_default_154 = rotary_embedding_default_30 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_8 = torch.ops.aten.split_with_sizes.default(mm_4, [4096, 1024, 1024], -1)
        getitem_43: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_8[1];  split_with_sizes_8 = None
        split_with_sizes_9 = torch.ops.aten.split_with_sizes.default(mm_4, [4096, 1024, 1024], -1)
        getitem_47: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_9[2];  split_with_sizes_9 = None
        view_14: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_43, [-1, 8, 128]);  getitem_43 = None
        view_15: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_47, [-1, 8, 128]);  getitem_47 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_3: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg26_1, 1)
        sym_stride_int_4: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg26_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_3: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg26_1, 0, 1)
        sym_storage_offset_default_1: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_3);  select_3 = None

        # No stacktrace found for following nodes
        as_strided_default_151: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg26_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_3, sym_stride_int_4, arg12_1, 1], 0)
        as_strided_default_152: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg26_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_3, sym_stride_int_4, arg12_1, 1], sym_storage_offset_default_1);  sym_stride_int_3 = sym_stride_int_4 = sym_storage_offset_default_1 = None
        reshape_and_cache_flash_default_30 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_14, view_15, as_strided_default_151, as_strided_default_152, arg15_1, 'auto', 1.0, 1.0);  view_14 = view_15 = as_strided_default_151 = as_strided_default_152 = reshape_and_cache_flash_default_30 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_3: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_7: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_3, [0, 1, 2]);  empty_3 = permute_7 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_11 = torch.ops.aten.split_with_sizes.default(mm_4, [4096, 1024, 1024], -1)
        getitem_53: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_11[0];  split_with_sizes_11 = None
        split_with_sizes_12 = torch.ops.aten.split_with_sizes.default(mm_4, [4096, 1024, 1024], -1)
        getitem_57: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_12[1];  split_with_sizes_12 = None
        split_with_sizes_13 = torch.ops.aten.split_with_sizes.default(mm_4, [4096, 1024, 1024], -1);  mm_4 = None
        getitem_61: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_13[2];  split_with_sizes_13 = None
        view_17: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_53, [-1, 32, 128]);  getitem_53 = None
        slice_10: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_17, 0, 0, arg16_1);  view_17 = None
        view_18: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_57, [-1, 8, 128]);  getitem_57 = None
        slice_11: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_18, 0, 0, arg16_1);  view_18 = None
        view_19: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_61, [-1, 8, 128]);  getitem_61 = None
        slice_12: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_19, 0, 0, arg16_1);  view_19 = None
        flash_attn_varlen_func_1: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_10, slice_11, slice_12, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_10 = slice_11 = slice_12 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_21: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_1, [arg0_1, 4096]);  flash_attn_varlen_func_1 = None
        permute_8: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg27_1, [1, 0]);  arg27_1 = None
        mm_5: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_21, permute_8);  view_21 = permute_8 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_61 = torch.ops._C.fused_add_rms_norm.default(mm_5, embedding, arg28_1, 1e-05);  arg28_1 = fused_add_rms_norm_default_61 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_9: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg29_1, [1, 0]);  arg29_1 = None
        mm_6: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_5, permute_9);  mm_5 = permute_9 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_4: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_150: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_4, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_30 = torch.ops._C.silu_and_mul.default(as_strided_default_150, mm_6);  as_strided_default_150 = mm_6 = silu_and_mul_default_30 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_10: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg30_1, [1, 0]);  arg30_1 = None
        mm_7: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_4, permute_10);  empty_4 = permute_10 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_60 = torch.ops._C.fused_add_rms_norm.default(mm_7, embedding, arg31_1, 1e-05);  arg31_1 = fused_add_rms_norm_default_60 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_11: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg32_1, [1, 0]);  arg32_1 = None
        mm_8: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_7, permute_11);  mm_7 = permute_11 = None

        # No stacktrace found for following nodes
        as_strided_default_148: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_8, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_149: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_8, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_29 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_148, as_strided_default_149, 128, arg5_1, True);  as_strided_default_148 = as_strided_default_149 = rotary_embedding_default_29 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_15 = torch.ops.aten.split_with_sizes.default(mm_8, [4096, 1024, 1024], -1)
        getitem_76: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_15[1];  split_with_sizes_15 = None
        split_with_sizes_16 = torch.ops.aten.split_with_sizes.default(mm_8, [4096, 1024, 1024], -1)
        getitem_80: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_16[2];  split_with_sizes_16 = None
        view_25: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_76, [-1, 8, 128]);  getitem_76 = None
        view_26: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_80, [-1, 8, 128]);  getitem_80 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_6: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg33_1, 1)
        sym_stride_int_7: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg33_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_5: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg33_1, 0, 1)
        sym_storage_offset_default_2: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_5);  select_5 = None

        # No stacktrace found for following nodes
        as_strided_default_146: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg33_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_6, sym_stride_int_7, arg12_1, 1], 0)
        as_strided_default_147: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg33_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_6, sym_stride_int_7, arg12_1, 1], sym_storage_offset_default_2);  sym_stride_int_6 = sym_stride_int_7 = sym_storage_offset_default_2 = None
        reshape_and_cache_flash_default_29 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_25, view_26, as_strided_default_146, as_strided_default_147, arg15_1, 'auto', 1.0, 1.0);  view_25 = view_26 = as_strided_default_146 = as_strided_default_147 = reshape_and_cache_flash_default_29 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_5: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_12: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_5, [0, 1, 2]);  empty_5 = permute_12 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_18 = torch.ops.aten.split_with_sizes.default(mm_8, [4096, 1024, 1024], -1)
        getitem_86: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_18[0];  split_with_sizes_18 = None
        split_with_sizes_19 = torch.ops.aten.split_with_sizes.default(mm_8, [4096, 1024, 1024], -1)
        getitem_90: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_19[1];  split_with_sizes_19 = None
        split_with_sizes_20 = torch.ops.aten.split_with_sizes.default(mm_8, [4096, 1024, 1024], -1);  mm_8 = None
        getitem_94: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_20[2];  split_with_sizes_20 = None
        view_28: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_86, [-1, 32, 128]);  getitem_86 = None
        slice_16: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_28, 0, 0, arg16_1);  view_28 = None
        view_29: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_90, [-1, 8, 128]);  getitem_90 = None
        slice_17: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_29, 0, 0, arg16_1);  view_29 = None
        view_30: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_94, [-1, 8, 128]);  getitem_94 = None
        slice_18: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_30, 0, 0, arg16_1);  view_30 = None
        flash_attn_varlen_func_2: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_16, slice_17, slice_18, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_16 = slice_17 = slice_18 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_32: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_2, [arg0_1, 4096]);  flash_attn_varlen_func_2 = None
        permute_13: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg34_1, [1, 0]);  arg34_1 = None
        mm_9: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_13);  view_32 = permute_13 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_59 = torch.ops._C.fused_add_rms_norm.default(mm_9, embedding, arg35_1, 1e-05);  arg35_1 = fused_add_rms_norm_default_59 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_14: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg36_1, [1, 0]);  arg36_1 = None
        mm_10: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_9, permute_14);  mm_9 = permute_14 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_6: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_145: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_6, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_29 = torch.ops._C.silu_and_mul.default(as_strided_default_145, mm_10);  as_strided_default_145 = mm_10 = silu_and_mul_default_29 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_15: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg37_1, [1, 0]);  arg37_1 = None
        mm_11: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_6, permute_15);  empty_6 = permute_15 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_58 = torch.ops._C.fused_add_rms_norm.default(mm_11, embedding, arg38_1, 1e-05);  arg38_1 = fused_add_rms_norm_default_58 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_16: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg39_1, [1, 0]);  arg39_1 = None
        mm_12: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_11, permute_16);  mm_11 = permute_16 = None

        # No stacktrace found for following nodes
        as_strided_default_143: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_12, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_144: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_12, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_28 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_143, as_strided_default_144, 128, arg5_1, True);  as_strided_default_143 = as_strided_default_144 = rotary_embedding_default_28 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_22 = torch.ops.aten.split_with_sizes.default(mm_12, [4096, 1024, 1024], -1)
        getitem_109: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_22[1];  split_with_sizes_22 = None
        split_with_sizes_23 = torch.ops.aten.split_with_sizes.default(mm_12, [4096, 1024, 1024], -1)
        getitem_113: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_23[2];  split_with_sizes_23 = None
        view_36: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_109, [-1, 8, 128]);  getitem_109 = None
        view_37: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_113, [-1, 8, 128]);  getitem_113 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_9: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg40_1, 1)
        sym_stride_int_10: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg40_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_7: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg40_1, 0, 1)
        sym_storage_offset_default_3: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_7);  select_7 = None

        # No stacktrace found for following nodes
        as_strided_default_141: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg40_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_9, sym_stride_int_10, arg12_1, 1], 0)
        as_strided_default_142: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg40_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_9, sym_stride_int_10, arg12_1, 1], sym_storage_offset_default_3);  sym_stride_int_9 = sym_stride_int_10 = sym_storage_offset_default_3 = None
        reshape_and_cache_flash_default_28 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_36, view_37, as_strided_default_141, as_strided_default_142, arg15_1, 'auto', 1.0, 1.0);  view_36 = view_37 = as_strided_default_141 = as_strided_default_142 = reshape_and_cache_flash_default_28 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_7: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_17: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_7, [0, 1, 2]);  empty_7 = permute_17 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_25 = torch.ops.aten.split_with_sizes.default(mm_12, [4096, 1024, 1024], -1)
        getitem_119: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_25[0];  split_with_sizes_25 = None
        split_with_sizes_26 = torch.ops.aten.split_with_sizes.default(mm_12, [4096, 1024, 1024], -1)
        getitem_123: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_26[1];  split_with_sizes_26 = None
        split_with_sizes_27 = torch.ops.aten.split_with_sizes.default(mm_12, [4096, 1024, 1024], -1);  mm_12 = None
        getitem_127: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_27[2];  split_with_sizes_27 = None
        view_39: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_119, [-1, 32, 128]);  getitem_119 = None
        slice_22: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_39, 0, 0, arg16_1);  view_39 = None
        view_40: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_123, [-1, 8, 128]);  getitem_123 = None
        slice_23: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_40, 0, 0, arg16_1);  view_40 = None
        view_41: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_127, [-1, 8, 128]);  getitem_127 = None
        slice_24: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_41, 0, 0, arg16_1);  view_41 = None
        flash_attn_varlen_func_3: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_22, slice_23, slice_24, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_22 = slice_23 = slice_24 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_43: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_3, [arg0_1, 4096]);  flash_attn_varlen_func_3 = None
        permute_18: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg41_1, [1, 0]);  arg41_1 = None
        mm_13: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_43, permute_18);  view_43 = permute_18 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_57 = torch.ops._C.fused_add_rms_norm.default(mm_13, embedding, arg42_1, 1e-05);  arg42_1 = fused_add_rms_norm_default_57 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_19: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg43_1, [1, 0]);  arg43_1 = None
        mm_14: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_13, permute_19);  mm_13 = permute_19 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_8: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_140: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_8, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_28 = torch.ops._C.silu_and_mul.default(as_strided_default_140, mm_14);  as_strided_default_140 = mm_14 = silu_and_mul_default_28 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_20: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg44_1, [1, 0]);  arg44_1 = None
        mm_15: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_8, permute_20);  empty_8 = permute_20 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_56 = torch.ops._C.fused_add_rms_norm.default(mm_15, embedding, arg45_1, 1e-05);  arg45_1 = fused_add_rms_norm_default_56 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_21: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg46_1, [1, 0]);  arg46_1 = None
        mm_16: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_15, permute_21);  mm_15 = permute_21 = None

        # No stacktrace found for following nodes
        as_strided_default_138: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_16, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_139: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_16, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_27 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_138, as_strided_default_139, 128, arg5_1, True);  as_strided_default_138 = as_strided_default_139 = rotary_embedding_default_27 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_29 = torch.ops.aten.split_with_sizes.default(mm_16, [4096, 1024, 1024], -1)
        getitem_142: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_29[1];  split_with_sizes_29 = None
        split_with_sizes_30 = torch.ops.aten.split_with_sizes.default(mm_16, [4096, 1024, 1024], -1)
        getitem_146: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_30[2];  split_with_sizes_30 = None
        view_47: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_142, [-1, 8, 128]);  getitem_142 = None
        view_48: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_146, [-1, 8, 128]);  getitem_146 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_12: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg47_1, 1)
        sym_stride_int_13: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg47_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_9: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg47_1, 0, 1)
        sym_storage_offset_default_4: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_9);  select_9 = None

        # No stacktrace found for following nodes
        as_strided_default_136: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg47_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_12, sym_stride_int_13, arg12_1, 1], 0)
        as_strided_default_137: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg47_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_12, sym_stride_int_13, arg12_1, 1], sym_storage_offset_default_4);  sym_stride_int_12 = sym_stride_int_13 = sym_storage_offset_default_4 = None
        reshape_and_cache_flash_default_27 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_47, view_48, as_strided_default_136, as_strided_default_137, arg15_1, 'auto', 1.0, 1.0);  view_47 = view_48 = as_strided_default_136 = as_strided_default_137 = reshape_and_cache_flash_default_27 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_9: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_22: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_9, [0, 1, 2]);  empty_9 = permute_22 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_32 = torch.ops.aten.split_with_sizes.default(mm_16, [4096, 1024, 1024], -1)
        getitem_152: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_32[0];  split_with_sizes_32 = None
        split_with_sizes_33 = torch.ops.aten.split_with_sizes.default(mm_16, [4096, 1024, 1024], -1)
        getitem_156: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_33[1];  split_with_sizes_33 = None
        split_with_sizes_34 = torch.ops.aten.split_with_sizes.default(mm_16, [4096, 1024, 1024], -1);  mm_16 = None
        getitem_160: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_34[2];  split_with_sizes_34 = None
        view_50: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_152, [-1, 32, 128]);  getitem_152 = None
        slice_28: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_50, 0, 0, arg16_1);  view_50 = None
        view_51: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_156, [-1, 8, 128]);  getitem_156 = None
        slice_29: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_51, 0, 0, arg16_1);  view_51 = None
        view_52: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_160, [-1, 8, 128]);  getitem_160 = None
        slice_30: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_52, 0, 0, arg16_1);  view_52 = None
        flash_attn_varlen_func_4: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_28, slice_29, slice_30, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_28 = slice_29 = slice_30 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_54: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_4, [arg0_1, 4096]);  flash_attn_varlen_func_4 = None
        permute_23: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg48_1, [1, 0]);  arg48_1 = None
        mm_17: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_54, permute_23);  view_54 = permute_23 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_55 = torch.ops._C.fused_add_rms_norm.default(mm_17, embedding, arg49_1, 1e-05);  arg49_1 = fused_add_rms_norm_default_55 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_24: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg50_1, [1, 0]);  arg50_1 = None
        mm_18: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_17, permute_24);  mm_17 = permute_24 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_10: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_135: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_10, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_27 = torch.ops._C.silu_and_mul.default(as_strided_default_135, mm_18);  as_strided_default_135 = mm_18 = silu_and_mul_default_27 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_25: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg51_1, [1, 0]);  arg51_1 = None
        mm_19: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_10, permute_25);  empty_10 = permute_25 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_54 = torch.ops._C.fused_add_rms_norm.default(mm_19, embedding, arg52_1, 1e-05);  arg52_1 = fused_add_rms_norm_default_54 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_26: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg53_1, [1, 0]);  arg53_1 = None
        mm_20: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_19, permute_26);  mm_19 = permute_26 = None

        # No stacktrace found for following nodes
        as_strided_default_133: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_20, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_134: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_20, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_26 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_133, as_strided_default_134, 128, arg5_1, True);  as_strided_default_133 = as_strided_default_134 = rotary_embedding_default_26 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_36 = torch.ops.aten.split_with_sizes.default(mm_20, [4096, 1024, 1024], -1)
        getitem_175: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_36[1];  split_with_sizes_36 = None
        split_with_sizes_37 = torch.ops.aten.split_with_sizes.default(mm_20, [4096, 1024, 1024], -1)
        getitem_179: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_37[2];  split_with_sizes_37 = None
        view_58: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_175, [-1, 8, 128]);  getitem_175 = None
        view_59: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_179, [-1, 8, 128]);  getitem_179 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_15: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg54_1, 1)
        sym_stride_int_16: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg54_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_11: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg54_1, 0, 1)
        sym_storage_offset_default_5: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_11);  select_11 = None

        # No stacktrace found for following nodes
        as_strided_default_131: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg54_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_15, sym_stride_int_16, arg12_1, 1], 0)
        as_strided_default_132: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg54_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_15, sym_stride_int_16, arg12_1, 1], sym_storage_offset_default_5);  sym_stride_int_15 = sym_stride_int_16 = sym_storage_offset_default_5 = None
        reshape_and_cache_flash_default_26 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_58, view_59, as_strided_default_131, as_strided_default_132, arg15_1, 'auto', 1.0, 1.0);  view_58 = view_59 = as_strided_default_131 = as_strided_default_132 = reshape_and_cache_flash_default_26 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_11: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_27: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_11, [0, 1, 2]);  empty_11 = permute_27 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_39 = torch.ops.aten.split_with_sizes.default(mm_20, [4096, 1024, 1024], -1)
        getitem_185: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_39[0];  split_with_sizes_39 = None
        split_with_sizes_40 = torch.ops.aten.split_with_sizes.default(mm_20, [4096, 1024, 1024], -1)
        getitem_189: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_40[1];  split_with_sizes_40 = None
        split_with_sizes_41 = torch.ops.aten.split_with_sizes.default(mm_20, [4096, 1024, 1024], -1);  mm_20 = None
        getitem_193: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_41[2];  split_with_sizes_41 = None
        view_61: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_185, [-1, 32, 128]);  getitem_185 = None
        slice_34: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_61, 0, 0, arg16_1);  view_61 = None
        view_62: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_189, [-1, 8, 128]);  getitem_189 = None
        slice_35: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_62, 0, 0, arg16_1);  view_62 = None
        view_63: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_193, [-1, 8, 128]);  getitem_193 = None
        slice_36: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_63, 0, 0, arg16_1);  view_63 = None
        flash_attn_varlen_func_5: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_34, slice_35, slice_36, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_34 = slice_35 = slice_36 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_65: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_5, [arg0_1, 4096]);  flash_attn_varlen_func_5 = None
        permute_28: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg55_1, [1, 0]);  arg55_1 = None
        mm_21: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_65, permute_28);  view_65 = permute_28 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_53 = torch.ops._C.fused_add_rms_norm.default(mm_21, embedding, arg56_1, 1e-05);  arg56_1 = fused_add_rms_norm_default_53 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_29: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg57_1, [1, 0]);  arg57_1 = None
        mm_22: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_21, permute_29);  mm_21 = permute_29 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_12: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_130: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_12, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_26 = torch.ops._C.silu_and_mul.default(as_strided_default_130, mm_22);  as_strided_default_130 = mm_22 = silu_and_mul_default_26 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_30: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg58_1, [1, 0]);  arg58_1 = None
        mm_23: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_12, permute_30);  empty_12 = permute_30 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_52 = torch.ops._C.fused_add_rms_norm.default(mm_23, embedding, arg59_1, 1e-05);  arg59_1 = fused_add_rms_norm_default_52 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_31: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg60_1, [1, 0]);  arg60_1 = None
        mm_24: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_23, permute_31);  mm_23 = permute_31 = None

        # No stacktrace found for following nodes
        as_strided_default_128: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_24, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_129: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_24, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_25 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_128, as_strided_default_129, 128, arg5_1, True);  as_strided_default_128 = as_strided_default_129 = rotary_embedding_default_25 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_43 = torch.ops.aten.split_with_sizes.default(mm_24, [4096, 1024, 1024], -1)
        getitem_208: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_43[1];  split_with_sizes_43 = None
        split_with_sizes_44 = torch.ops.aten.split_with_sizes.default(mm_24, [4096, 1024, 1024], -1)
        getitem_212: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_44[2];  split_with_sizes_44 = None
        view_69: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_208, [-1, 8, 128]);  getitem_208 = None
        view_70: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_212, [-1, 8, 128]);  getitem_212 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_18: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg61_1, 1)
        sym_stride_int_19: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg61_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_13: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg61_1, 0, 1)
        sym_storage_offset_default_6: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_13);  select_13 = None

        # No stacktrace found for following nodes
        as_strided_default_126: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg61_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_18, sym_stride_int_19, arg12_1, 1], 0)
        as_strided_default_127: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg61_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_18, sym_stride_int_19, arg12_1, 1], sym_storage_offset_default_6);  sym_stride_int_18 = sym_stride_int_19 = sym_storage_offset_default_6 = None
        reshape_and_cache_flash_default_25 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_69, view_70, as_strided_default_126, as_strided_default_127, arg15_1, 'auto', 1.0, 1.0);  view_69 = view_70 = as_strided_default_126 = as_strided_default_127 = reshape_and_cache_flash_default_25 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_13: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_32: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_13, [0, 1, 2]);  empty_13 = permute_32 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_46 = torch.ops.aten.split_with_sizes.default(mm_24, [4096, 1024, 1024], -1)
        getitem_218: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_46[0];  split_with_sizes_46 = None
        split_with_sizes_47 = torch.ops.aten.split_with_sizes.default(mm_24, [4096, 1024, 1024], -1)
        getitem_222: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_47[1];  split_with_sizes_47 = None
        split_with_sizes_48 = torch.ops.aten.split_with_sizes.default(mm_24, [4096, 1024, 1024], -1);  mm_24 = None
        getitem_226: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_48[2];  split_with_sizes_48 = None
        view_72: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_218, [-1, 32, 128]);  getitem_218 = None
        slice_40: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_72, 0, 0, arg16_1);  view_72 = None
        view_73: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_222, [-1, 8, 128]);  getitem_222 = None
        slice_41: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_73, 0, 0, arg16_1);  view_73 = None
        view_74: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_226, [-1, 8, 128]);  getitem_226 = None
        slice_42: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_74, 0, 0, arg16_1);  view_74 = None
        flash_attn_varlen_func_6: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_40, slice_41, slice_42, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_40 = slice_41 = slice_42 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_76: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_6, [arg0_1, 4096]);  flash_attn_varlen_func_6 = None
        permute_33: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg62_1, [1, 0]);  arg62_1 = None
        mm_25: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_76, permute_33);  view_76 = permute_33 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_51 = torch.ops._C.fused_add_rms_norm.default(mm_25, embedding, arg63_1, 1e-05);  arg63_1 = fused_add_rms_norm_default_51 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_34: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg64_1, [1, 0]);  arg64_1 = None
        mm_26: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_25, permute_34);  mm_25 = permute_34 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_14: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_125: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_14, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_25 = torch.ops._C.silu_and_mul.default(as_strided_default_125, mm_26);  as_strided_default_125 = mm_26 = silu_and_mul_default_25 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_35: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg65_1, [1, 0]);  arg65_1 = None
        mm_27: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_14, permute_35);  empty_14 = permute_35 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_50 = torch.ops._C.fused_add_rms_norm.default(mm_27, embedding, arg66_1, 1e-05);  arg66_1 = fused_add_rms_norm_default_50 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_36: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg67_1, [1, 0]);  arg67_1 = None
        mm_28: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_27, permute_36);  mm_27 = permute_36 = None

        # No stacktrace found for following nodes
        as_strided_default_123: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_28, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_124: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_28, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_24 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_123, as_strided_default_124, 128, arg5_1, True);  as_strided_default_123 = as_strided_default_124 = rotary_embedding_default_24 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_50 = torch.ops.aten.split_with_sizes.default(mm_28, [4096, 1024, 1024], -1)
        getitem_241: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_50[1];  split_with_sizes_50 = None
        split_with_sizes_51 = torch.ops.aten.split_with_sizes.default(mm_28, [4096, 1024, 1024], -1)
        getitem_245: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_51[2];  split_with_sizes_51 = None
        view_80: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_241, [-1, 8, 128]);  getitem_241 = None
        view_81: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_245, [-1, 8, 128]);  getitem_245 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_21: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg68_1, 1)
        sym_stride_int_22: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg68_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_15: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg68_1, 0, 1)
        sym_storage_offset_default_7: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_15);  select_15 = None

        # No stacktrace found for following nodes
        as_strided_default_121: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg68_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_21, sym_stride_int_22, arg12_1, 1], 0)
        as_strided_default_122: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg68_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_21, sym_stride_int_22, arg12_1, 1], sym_storage_offset_default_7);  sym_stride_int_21 = sym_stride_int_22 = sym_storage_offset_default_7 = None
        reshape_and_cache_flash_default_24 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_80, view_81, as_strided_default_121, as_strided_default_122, arg15_1, 'auto', 1.0, 1.0);  view_80 = view_81 = as_strided_default_121 = as_strided_default_122 = reshape_and_cache_flash_default_24 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_15: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_37: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_15, [0, 1, 2]);  empty_15 = permute_37 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_53 = torch.ops.aten.split_with_sizes.default(mm_28, [4096, 1024, 1024], -1)
        getitem_251: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_53[0];  split_with_sizes_53 = None
        split_with_sizes_54 = torch.ops.aten.split_with_sizes.default(mm_28, [4096, 1024, 1024], -1)
        getitem_255: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_54[1];  split_with_sizes_54 = None
        split_with_sizes_55 = torch.ops.aten.split_with_sizes.default(mm_28, [4096, 1024, 1024], -1);  mm_28 = None
        getitem_259: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_55[2];  split_with_sizes_55 = None
        view_83: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_251, [-1, 32, 128]);  getitem_251 = None
        slice_46: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_83, 0, 0, arg16_1);  view_83 = None
        view_84: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_255, [-1, 8, 128]);  getitem_255 = None
        slice_47: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_84, 0, 0, arg16_1);  view_84 = None
        view_85: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_259, [-1, 8, 128]);  getitem_259 = None
        slice_48: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_85, 0, 0, arg16_1);  view_85 = None
        flash_attn_varlen_func_7: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_46, slice_47, slice_48, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_46 = slice_47 = slice_48 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_87: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_7, [arg0_1, 4096]);  flash_attn_varlen_func_7 = None
        permute_38: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg69_1, [1, 0]);  arg69_1 = None
        mm_29: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_87, permute_38);  view_87 = permute_38 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_49 = torch.ops._C.fused_add_rms_norm.default(mm_29, embedding, arg70_1, 1e-05);  arg70_1 = fused_add_rms_norm_default_49 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_39: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg71_1, [1, 0]);  arg71_1 = None
        mm_30: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_29, permute_39);  mm_29 = permute_39 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_16: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_120: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_16, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_24 = torch.ops._C.silu_and_mul.default(as_strided_default_120, mm_30);  as_strided_default_120 = mm_30 = silu_and_mul_default_24 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_40: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg72_1, [1, 0]);  arg72_1 = None
        mm_31: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_16, permute_40);  empty_16 = permute_40 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_48 = torch.ops._C.fused_add_rms_norm.default(mm_31, embedding, arg73_1, 1e-05);  arg73_1 = fused_add_rms_norm_default_48 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_41: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg74_1, [1, 0]);  arg74_1 = None
        mm_32: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_31, permute_41);  mm_31 = permute_41 = None

        # No stacktrace found for following nodes
        as_strided_default_118: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_32, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_119: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_32, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_23 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_118, as_strided_default_119, 128, arg5_1, True);  as_strided_default_118 = as_strided_default_119 = rotary_embedding_default_23 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_57 = torch.ops.aten.split_with_sizes.default(mm_32, [4096, 1024, 1024], -1)
        getitem_274: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_57[1];  split_with_sizes_57 = None
        split_with_sizes_58 = torch.ops.aten.split_with_sizes.default(mm_32, [4096, 1024, 1024], -1)
        getitem_278: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_58[2];  split_with_sizes_58 = None
        view_91: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_274, [-1, 8, 128]);  getitem_274 = None
        view_92: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_278, [-1, 8, 128]);  getitem_278 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_24: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg75_1, 1)
        sym_stride_int_25: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg75_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_17: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg75_1, 0, 1)
        sym_storage_offset_default_8: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_17);  select_17 = None

        # No stacktrace found for following nodes
        as_strided_default_116: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg75_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_24, sym_stride_int_25, arg12_1, 1], 0)
        as_strided_default_117: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg75_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_24, sym_stride_int_25, arg12_1, 1], sym_storage_offset_default_8);  sym_stride_int_24 = sym_stride_int_25 = sym_storage_offset_default_8 = None
        reshape_and_cache_flash_default_23 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_91, view_92, as_strided_default_116, as_strided_default_117, arg15_1, 'auto', 1.0, 1.0);  view_91 = view_92 = as_strided_default_116 = as_strided_default_117 = reshape_and_cache_flash_default_23 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_17: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_42: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_17, [0, 1, 2]);  empty_17 = permute_42 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_60 = torch.ops.aten.split_with_sizes.default(mm_32, [4096, 1024, 1024], -1)
        getitem_284: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_60[0];  split_with_sizes_60 = None
        split_with_sizes_61 = torch.ops.aten.split_with_sizes.default(mm_32, [4096, 1024, 1024], -1)
        getitem_288: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_61[1];  split_with_sizes_61 = None
        split_with_sizes_62 = torch.ops.aten.split_with_sizes.default(mm_32, [4096, 1024, 1024], -1);  mm_32 = None
        getitem_292: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_62[2];  split_with_sizes_62 = None
        view_94: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_284, [-1, 32, 128]);  getitem_284 = None
        slice_52: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_94, 0, 0, arg16_1);  view_94 = None
        view_95: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_288, [-1, 8, 128]);  getitem_288 = None
        slice_53: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_95, 0, 0, arg16_1);  view_95 = None
        view_96: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_292, [-1, 8, 128]);  getitem_292 = None
        slice_54: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_96, 0, 0, arg16_1);  view_96 = None
        flash_attn_varlen_func_8: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_52, slice_53, slice_54, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_52 = slice_53 = slice_54 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_98: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_8, [arg0_1, 4096]);  flash_attn_varlen_func_8 = None
        permute_43: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg76_1, [1, 0]);  arg76_1 = None
        mm_33: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_98, permute_43);  view_98 = permute_43 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_47 = torch.ops._C.fused_add_rms_norm.default(mm_33, embedding, arg77_1, 1e-05);  arg77_1 = fused_add_rms_norm_default_47 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_44: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg78_1, [1, 0]);  arg78_1 = None
        mm_34: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_33, permute_44);  mm_33 = permute_44 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_18: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_115: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_18, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_23 = torch.ops._C.silu_and_mul.default(as_strided_default_115, mm_34);  as_strided_default_115 = mm_34 = silu_and_mul_default_23 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_45: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg79_1, [1, 0]);  arg79_1 = None
        mm_35: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_18, permute_45);  empty_18 = permute_45 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_46 = torch.ops._C.fused_add_rms_norm.default(mm_35, embedding, arg80_1, 1e-05);  arg80_1 = fused_add_rms_norm_default_46 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_46: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg81_1, [1, 0]);  arg81_1 = None
        mm_36: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_35, permute_46);  mm_35 = permute_46 = None

        # No stacktrace found for following nodes
        as_strided_default_113: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_36, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_114: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_36, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_22 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_113, as_strided_default_114, 128, arg5_1, True);  as_strided_default_113 = as_strided_default_114 = rotary_embedding_default_22 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_64 = torch.ops.aten.split_with_sizes.default(mm_36, [4096, 1024, 1024], -1)
        getitem_307: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_64[1];  split_with_sizes_64 = None
        split_with_sizes_65 = torch.ops.aten.split_with_sizes.default(mm_36, [4096, 1024, 1024], -1)
        getitem_311: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_65[2];  split_with_sizes_65 = None
        view_102: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_307, [-1, 8, 128]);  getitem_307 = None
        view_103: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_311, [-1, 8, 128]);  getitem_311 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_27: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg82_1, 1)
        sym_stride_int_28: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg82_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_19: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg82_1, 0, 1)
        sym_storage_offset_default_9: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_19);  select_19 = None

        # No stacktrace found for following nodes
        as_strided_default_111: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg82_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_27, sym_stride_int_28, arg12_1, 1], 0)
        as_strided_default_112: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg82_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_27, sym_stride_int_28, arg12_1, 1], sym_storage_offset_default_9);  sym_stride_int_27 = sym_stride_int_28 = sym_storage_offset_default_9 = None
        reshape_and_cache_flash_default_22 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_102, view_103, as_strided_default_111, as_strided_default_112, arg15_1, 'auto', 1.0, 1.0);  view_102 = view_103 = as_strided_default_111 = as_strided_default_112 = reshape_and_cache_flash_default_22 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_19: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_47: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_19, [0, 1, 2]);  empty_19 = permute_47 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_67 = torch.ops.aten.split_with_sizes.default(mm_36, [4096, 1024, 1024], -1)
        getitem_317: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_67[0];  split_with_sizes_67 = None
        split_with_sizes_68 = torch.ops.aten.split_with_sizes.default(mm_36, [4096, 1024, 1024], -1)
        getitem_321: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_68[1];  split_with_sizes_68 = None
        split_with_sizes_69 = torch.ops.aten.split_with_sizes.default(mm_36, [4096, 1024, 1024], -1);  mm_36 = None
        getitem_325: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_69[2];  split_with_sizes_69 = None
        view_105: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_317, [-1, 32, 128]);  getitem_317 = None
        slice_58: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_105, 0, 0, arg16_1);  view_105 = None
        view_106: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_321, [-1, 8, 128]);  getitem_321 = None
        slice_59: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_106, 0, 0, arg16_1);  view_106 = None
        view_107: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_325, [-1, 8, 128]);  getitem_325 = None
        slice_60: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_107, 0, 0, arg16_1);  view_107 = None
        flash_attn_varlen_func_9: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_58, slice_59, slice_60, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_58 = slice_59 = slice_60 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_109: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_9, [arg0_1, 4096]);  flash_attn_varlen_func_9 = None
        permute_48: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg83_1, [1, 0]);  arg83_1 = None
        mm_37: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_109, permute_48);  view_109 = permute_48 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_45 = torch.ops._C.fused_add_rms_norm.default(mm_37, embedding, arg84_1, 1e-05);  arg84_1 = fused_add_rms_norm_default_45 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_49: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg85_1, [1, 0]);  arg85_1 = None
        mm_38: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_37, permute_49);  mm_37 = permute_49 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_20: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_110: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_20, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_22 = torch.ops._C.silu_and_mul.default(as_strided_default_110, mm_38);  as_strided_default_110 = mm_38 = silu_and_mul_default_22 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_50: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg86_1, [1, 0]);  arg86_1 = None
        mm_39: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_20, permute_50);  empty_20 = permute_50 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_44 = torch.ops._C.fused_add_rms_norm.default(mm_39, embedding, arg87_1, 1e-05);  arg87_1 = fused_add_rms_norm_default_44 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_51: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg88_1, [1, 0]);  arg88_1 = None
        mm_40: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_39, permute_51);  mm_39 = permute_51 = None

        # No stacktrace found for following nodes
        as_strided_default_108: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_40, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_109: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_40, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_21 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_108, as_strided_default_109, 128, arg5_1, True);  as_strided_default_108 = as_strided_default_109 = rotary_embedding_default_21 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_71 = torch.ops.aten.split_with_sizes.default(mm_40, [4096, 1024, 1024], -1)
        getitem_340: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_71[1];  split_with_sizes_71 = None
        split_with_sizes_72 = torch.ops.aten.split_with_sizes.default(mm_40, [4096, 1024, 1024], -1)
        getitem_344: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_72[2];  split_with_sizes_72 = None
        view_113: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_340, [-1, 8, 128]);  getitem_340 = None
        view_114: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_344, [-1, 8, 128]);  getitem_344 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_30: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg89_1, 1)
        sym_stride_int_31: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg89_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_21: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg89_1, 0, 1)
        sym_storage_offset_default_10: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_21);  select_21 = None

        # No stacktrace found for following nodes
        as_strided_default_106: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg89_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_30, sym_stride_int_31, arg12_1, 1], 0)
        as_strided_default_107: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg89_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_30, sym_stride_int_31, arg12_1, 1], sym_storage_offset_default_10);  sym_stride_int_30 = sym_stride_int_31 = sym_storage_offset_default_10 = None
        reshape_and_cache_flash_default_21 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_113, view_114, as_strided_default_106, as_strided_default_107, arg15_1, 'auto', 1.0, 1.0);  view_113 = view_114 = as_strided_default_106 = as_strided_default_107 = reshape_and_cache_flash_default_21 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_21: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_52: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_21, [0, 1, 2]);  empty_21 = permute_52 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_74 = torch.ops.aten.split_with_sizes.default(mm_40, [4096, 1024, 1024], -1)
        getitem_350: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_74[0];  split_with_sizes_74 = None
        split_with_sizes_75 = torch.ops.aten.split_with_sizes.default(mm_40, [4096, 1024, 1024], -1)
        getitem_354: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_75[1];  split_with_sizes_75 = None
        split_with_sizes_76 = torch.ops.aten.split_with_sizes.default(mm_40, [4096, 1024, 1024], -1);  mm_40 = None
        getitem_358: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_76[2];  split_with_sizes_76 = None
        view_116: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_350, [-1, 32, 128]);  getitem_350 = None
        slice_64: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_116, 0, 0, arg16_1);  view_116 = None
        view_117: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_354, [-1, 8, 128]);  getitem_354 = None
        slice_65: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_117, 0, 0, arg16_1);  view_117 = None
        view_118: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_358, [-1, 8, 128]);  getitem_358 = None
        slice_66: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_118, 0, 0, arg16_1);  view_118 = None
        flash_attn_varlen_func_10: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_64, slice_65, slice_66, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_64 = slice_65 = slice_66 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_120: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_10, [arg0_1, 4096]);  flash_attn_varlen_func_10 = None
        permute_53: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg90_1, [1, 0]);  arg90_1 = None
        mm_41: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_120, permute_53);  view_120 = permute_53 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_43 = torch.ops._C.fused_add_rms_norm.default(mm_41, embedding, arg91_1, 1e-05);  arg91_1 = fused_add_rms_norm_default_43 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_54: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg92_1, [1, 0]);  arg92_1 = None
        mm_42: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_41, permute_54);  mm_41 = permute_54 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_22: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_105: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_22, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_21 = torch.ops._C.silu_and_mul.default(as_strided_default_105, mm_42);  as_strided_default_105 = mm_42 = silu_and_mul_default_21 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_55: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg93_1, [1, 0]);  arg93_1 = None
        mm_43: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_22, permute_55);  empty_22 = permute_55 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_42 = torch.ops._C.fused_add_rms_norm.default(mm_43, embedding, arg94_1, 1e-05);  arg94_1 = fused_add_rms_norm_default_42 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_56: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg95_1, [1, 0]);  arg95_1 = None
        mm_44: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_43, permute_56);  mm_43 = permute_56 = None

        # No stacktrace found for following nodes
        as_strided_default_103: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_44, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_104: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_44, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_20 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_103, as_strided_default_104, 128, arg5_1, True);  as_strided_default_103 = as_strided_default_104 = rotary_embedding_default_20 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_78 = torch.ops.aten.split_with_sizes.default(mm_44, [4096, 1024, 1024], -1)
        getitem_373: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_78[1];  split_with_sizes_78 = None
        split_with_sizes_79 = torch.ops.aten.split_with_sizes.default(mm_44, [4096, 1024, 1024], -1)
        getitem_377: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_79[2];  split_with_sizes_79 = None
        view_124: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_373, [-1, 8, 128]);  getitem_373 = None
        view_125: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_377, [-1, 8, 128]);  getitem_377 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_33: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg96_1, 1)
        sym_stride_int_34: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg96_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_23: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg96_1, 0, 1)
        sym_storage_offset_default_11: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_23);  select_23 = None

        # No stacktrace found for following nodes
        as_strided_default_101: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg96_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_33, sym_stride_int_34, arg12_1, 1], 0)
        as_strided_default_102: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg96_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_33, sym_stride_int_34, arg12_1, 1], sym_storage_offset_default_11);  sym_stride_int_33 = sym_stride_int_34 = sym_storage_offset_default_11 = None
        reshape_and_cache_flash_default_20 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_124, view_125, as_strided_default_101, as_strided_default_102, arg15_1, 'auto', 1.0, 1.0);  view_124 = view_125 = as_strided_default_101 = as_strided_default_102 = reshape_and_cache_flash_default_20 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_23: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_57: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_23, [0, 1, 2]);  empty_23 = permute_57 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_81 = torch.ops.aten.split_with_sizes.default(mm_44, [4096, 1024, 1024], -1)
        getitem_383: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_81[0];  split_with_sizes_81 = None
        split_with_sizes_82 = torch.ops.aten.split_with_sizes.default(mm_44, [4096, 1024, 1024], -1)
        getitem_387: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_82[1];  split_with_sizes_82 = None
        split_with_sizes_83 = torch.ops.aten.split_with_sizes.default(mm_44, [4096, 1024, 1024], -1);  mm_44 = None
        getitem_391: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_83[2];  split_with_sizes_83 = None
        view_127: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_383, [-1, 32, 128]);  getitem_383 = None
        slice_70: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_127, 0, 0, arg16_1);  view_127 = None
        view_128: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_387, [-1, 8, 128]);  getitem_387 = None
        slice_71: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_128, 0, 0, arg16_1);  view_128 = None
        view_129: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_391, [-1, 8, 128]);  getitem_391 = None
        slice_72: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_129, 0, 0, arg16_1);  view_129 = None
        flash_attn_varlen_func_11: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_70, slice_71, slice_72, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_70 = slice_71 = slice_72 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_131: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_11, [arg0_1, 4096]);  flash_attn_varlen_func_11 = None
        permute_58: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg97_1, [1, 0]);  arg97_1 = None
        mm_45: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_131, permute_58);  view_131 = permute_58 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_41 = torch.ops._C.fused_add_rms_norm.default(mm_45, embedding, arg98_1, 1e-05);  arg98_1 = fused_add_rms_norm_default_41 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_59: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg99_1, [1, 0]);  arg99_1 = None
        mm_46: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_45, permute_59);  mm_45 = permute_59 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_24: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_100: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_24, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_20 = torch.ops._C.silu_and_mul.default(as_strided_default_100, mm_46);  as_strided_default_100 = mm_46 = silu_and_mul_default_20 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_60: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg100_1, [1, 0]);  arg100_1 = None
        mm_47: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_24, permute_60);  empty_24 = permute_60 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_40 = torch.ops._C.fused_add_rms_norm.default(mm_47, embedding, arg101_1, 1e-05);  arg101_1 = fused_add_rms_norm_default_40 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_61: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg102_1, [1, 0]);  arg102_1 = None
        mm_48: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_47, permute_61);  mm_47 = permute_61 = None

        # No stacktrace found for following nodes
        as_strided_default_98: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_48, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_99: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_48, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_19 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_98, as_strided_default_99, 128, arg5_1, True);  as_strided_default_98 = as_strided_default_99 = rotary_embedding_default_19 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_85 = torch.ops.aten.split_with_sizes.default(mm_48, [4096, 1024, 1024], -1)
        getitem_406: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_85[1];  split_with_sizes_85 = None
        split_with_sizes_86 = torch.ops.aten.split_with_sizes.default(mm_48, [4096, 1024, 1024], -1)
        getitem_410: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_86[2];  split_with_sizes_86 = None
        view_135: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_406, [-1, 8, 128]);  getitem_406 = None
        view_136: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_410, [-1, 8, 128]);  getitem_410 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_36: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg103_1, 1)
        sym_stride_int_37: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg103_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_25: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg103_1, 0, 1)
        sym_storage_offset_default_12: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_25);  select_25 = None

        # No stacktrace found for following nodes
        as_strided_default_96: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg103_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_36, sym_stride_int_37, arg12_1, 1], 0)
        as_strided_default_97: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg103_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_36, sym_stride_int_37, arg12_1, 1], sym_storage_offset_default_12);  sym_stride_int_36 = sym_stride_int_37 = sym_storage_offset_default_12 = None
        reshape_and_cache_flash_default_19 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_135, view_136, as_strided_default_96, as_strided_default_97, arg15_1, 'auto', 1.0, 1.0);  view_135 = view_136 = as_strided_default_96 = as_strided_default_97 = reshape_and_cache_flash_default_19 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_25: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_62: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_25, [0, 1, 2]);  empty_25 = permute_62 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_88 = torch.ops.aten.split_with_sizes.default(mm_48, [4096, 1024, 1024], -1)
        getitem_416: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_88[0];  split_with_sizes_88 = None
        split_with_sizes_89 = torch.ops.aten.split_with_sizes.default(mm_48, [4096, 1024, 1024], -1)
        getitem_420: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_89[1];  split_with_sizes_89 = None
        split_with_sizes_90 = torch.ops.aten.split_with_sizes.default(mm_48, [4096, 1024, 1024], -1);  mm_48 = None
        getitem_424: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_90[2];  split_with_sizes_90 = None
        view_138: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_416, [-1, 32, 128]);  getitem_416 = None
        slice_76: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_138, 0, 0, arg16_1);  view_138 = None
        view_139: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_420, [-1, 8, 128]);  getitem_420 = None
        slice_77: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_139, 0, 0, arg16_1);  view_139 = None
        view_140: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_424, [-1, 8, 128]);  getitem_424 = None
        slice_78: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_140, 0, 0, arg16_1);  view_140 = None
        flash_attn_varlen_func_12: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_76, slice_77, slice_78, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_76 = slice_77 = slice_78 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_142: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_12, [arg0_1, 4096]);  flash_attn_varlen_func_12 = None
        permute_63: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg104_1, [1, 0]);  arg104_1 = None
        mm_49: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_142, permute_63);  view_142 = permute_63 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_39 = torch.ops._C.fused_add_rms_norm.default(mm_49, embedding, arg105_1, 1e-05);  arg105_1 = fused_add_rms_norm_default_39 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_64: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg106_1, [1, 0]);  arg106_1 = None
        mm_50: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_49, permute_64);  mm_49 = permute_64 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_26: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_95: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_26, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_19 = torch.ops._C.silu_and_mul.default(as_strided_default_95, mm_50);  as_strided_default_95 = mm_50 = silu_and_mul_default_19 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_65: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg107_1, [1, 0]);  arg107_1 = None
        mm_51: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_26, permute_65);  empty_26 = permute_65 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_38 = torch.ops._C.fused_add_rms_norm.default(mm_51, embedding, arg108_1, 1e-05);  arg108_1 = fused_add_rms_norm_default_38 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_66: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg109_1, [1, 0]);  arg109_1 = None
        mm_52: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_51, permute_66);  mm_51 = permute_66 = None

        # No stacktrace found for following nodes
        as_strided_default_93: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_52, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_94: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_52, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_18 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_93, as_strided_default_94, 128, arg5_1, True);  as_strided_default_93 = as_strided_default_94 = rotary_embedding_default_18 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_92 = torch.ops.aten.split_with_sizes.default(mm_52, [4096, 1024, 1024], -1)
        getitem_439: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_92[1];  split_with_sizes_92 = None
        split_with_sizes_93 = torch.ops.aten.split_with_sizes.default(mm_52, [4096, 1024, 1024], -1)
        getitem_443: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_93[2];  split_with_sizes_93 = None
        view_146: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_439, [-1, 8, 128]);  getitem_439 = None
        view_147: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_443, [-1, 8, 128]);  getitem_443 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_39: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg110_1, 1)
        sym_stride_int_40: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg110_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_27: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg110_1, 0, 1)
        sym_storage_offset_default_13: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_27);  select_27 = None

        # No stacktrace found for following nodes
        as_strided_default_91: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg110_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_39, sym_stride_int_40, arg12_1, 1], 0)
        as_strided_default_92: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg110_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_39, sym_stride_int_40, arg12_1, 1], sym_storage_offset_default_13);  sym_stride_int_39 = sym_stride_int_40 = sym_storage_offset_default_13 = None
        reshape_and_cache_flash_default_18 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_146, view_147, as_strided_default_91, as_strided_default_92, arg15_1, 'auto', 1.0, 1.0);  view_146 = view_147 = as_strided_default_91 = as_strided_default_92 = reshape_and_cache_flash_default_18 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_27: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_67: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_27, [0, 1, 2]);  empty_27 = permute_67 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_95 = torch.ops.aten.split_with_sizes.default(mm_52, [4096, 1024, 1024], -1)
        getitem_449: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_95[0];  split_with_sizes_95 = None
        split_with_sizes_96 = torch.ops.aten.split_with_sizes.default(mm_52, [4096, 1024, 1024], -1)
        getitem_453: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_96[1];  split_with_sizes_96 = None
        split_with_sizes_97 = torch.ops.aten.split_with_sizes.default(mm_52, [4096, 1024, 1024], -1);  mm_52 = None
        getitem_457: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_97[2];  split_with_sizes_97 = None
        view_149: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_449, [-1, 32, 128]);  getitem_449 = None
        slice_82: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_149, 0, 0, arg16_1);  view_149 = None
        view_150: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_453, [-1, 8, 128]);  getitem_453 = None
        slice_83: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_150, 0, 0, arg16_1);  view_150 = None
        view_151: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_457, [-1, 8, 128]);  getitem_457 = None
        slice_84: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_151, 0, 0, arg16_1);  view_151 = None
        flash_attn_varlen_func_13: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_82, slice_83, slice_84, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_82 = slice_83 = slice_84 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_153: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_13, [arg0_1, 4096]);  flash_attn_varlen_func_13 = None
        permute_68: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg111_1, [1, 0]);  arg111_1 = None
        mm_53: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_153, permute_68);  view_153 = permute_68 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_37 = torch.ops._C.fused_add_rms_norm.default(mm_53, embedding, arg112_1, 1e-05);  arg112_1 = fused_add_rms_norm_default_37 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_69: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg113_1, [1, 0]);  arg113_1 = None
        mm_54: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_53, permute_69);  mm_53 = permute_69 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_28: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_90: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_28, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_18 = torch.ops._C.silu_and_mul.default(as_strided_default_90, mm_54);  as_strided_default_90 = mm_54 = silu_and_mul_default_18 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_70: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg114_1, [1, 0]);  arg114_1 = None
        mm_55: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_28, permute_70);  empty_28 = permute_70 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_36 = torch.ops._C.fused_add_rms_norm.default(mm_55, embedding, arg115_1, 1e-05);  arg115_1 = fused_add_rms_norm_default_36 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_71: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg116_1, [1, 0]);  arg116_1 = None
        mm_56: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_55, permute_71);  mm_55 = permute_71 = None

        # No stacktrace found for following nodes
        as_strided_default_88: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_56, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_89: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_56, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_17 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_88, as_strided_default_89, 128, arg5_1, True);  as_strided_default_88 = as_strided_default_89 = rotary_embedding_default_17 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_99 = torch.ops.aten.split_with_sizes.default(mm_56, [4096, 1024, 1024], -1)
        getitem_472: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_99[1];  split_with_sizes_99 = None
        split_with_sizes_100 = torch.ops.aten.split_with_sizes.default(mm_56, [4096, 1024, 1024], -1)
        getitem_476: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_100[2];  split_with_sizes_100 = None
        view_157: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_472, [-1, 8, 128]);  getitem_472 = None
        view_158: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_476, [-1, 8, 128]);  getitem_476 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_42: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg117_1, 1)
        sym_stride_int_43: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg117_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_29: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg117_1, 0, 1)
        sym_storage_offset_default_14: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_29);  select_29 = None

        # No stacktrace found for following nodes
        as_strided_default_86: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg117_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_42, sym_stride_int_43, arg12_1, 1], 0)
        as_strided_default_87: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg117_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_42, sym_stride_int_43, arg12_1, 1], sym_storage_offset_default_14);  sym_stride_int_42 = sym_stride_int_43 = sym_storage_offset_default_14 = None
        reshape_and_cache_flash_default_17 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_157, view_158, as_strided_default_86, as_strided_default_87, arg15_1, 'auto', 1.0, 1.0);  view_157 = view_158 = as_strided_default_86 = as_strided_default_87 = reshape_and_cache_flash_default_17 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_29: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_72: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_29, [0, 1, 2]);  empty_29 = permute_72 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_102 = torch.ops.aten.split_with_sizes.default(mm_56, [4096, 1024, 1024], -1)
        getitem_482: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_102[0];  split_with_sizes_102 = None
        split_with_sizes_103 = torch.ops.aten.split_with_sizes.default(mm_56, [4096, 1024, 1024], -1)
        getitem_486: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_103[1];  split_with_sizes_103 = None
        split_with_sizes_104 = torch.ops.aten.split_with_sizes.default(mm_56, [4096, 1024, 1024], -1);  mm_56 = None
        getitem_490: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_104[2];  split_with_sizes_104 = None
        view_160: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_482, [-1, 32, 128]);  getitem_482 = None
        slice_88: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_160, 0, 0, arg16_1);  view_160 = None
        view_161: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_486, [-1, 8, 128]);  getitem_486 = None
        slice_89: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_161, 0, 0, arg16_1);  view_161 = None
        view_162: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_490, [-1, 8, 128]);  getitem_490 = None
        slice_90: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_162, 0, 0, arg16_1);  view_162 = None
        flash_attn_varlen_func_14: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_88, slice_89, slice_90, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_88 = slice_89 = slice_90 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_164: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_14, [arg0_1, 4096]);  flash_attn_varlen_func_14 = None
        permute_73: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg118_1, [1, 0]);  arg118_1 = None
        mm_57: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_164, permute_73);  view_164 = permute_73 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_35 = torch.ops._C.fused_add_rms_norm.default(mm_57, embedding, arg119_1, 1e-05);  arg119_1 = fused_add_rms_norm_default_35 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_74: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg120_1, [1, 0]);  arg120_1 = None
        mm_58: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_57, permute_74);  mm_57 = permute_74 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_30: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_85: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_30, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_17 = torch.ops._C.silu_and_mul.default(as_strided_default_85, mm_58);  as_strided_default_85 = mm_58 = silu_and_mul_default_17 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_75: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg121_1, [1, 0]);  arg121_1 = None
        mm_59: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_30, permute_75);  empty_30 = permute_75 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_34 = torch.ops._C.fused_add_rms_norm.default(mm_59, embedding, arg122_1, 1e-05);  arg122_1 = fused_add_rms_norm_default_34 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_76: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg123_1, [1, 0]);  arg123_1 = None
        mm_60: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_59, permute_76);  mm_59 = permute_76 = None

        # No stacktrace found for following nodes
        as_strided_default_83: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_60, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_84: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_60, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_16 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_83, as_strided_default_84, 128, arg5_1, True);  as_strided_default_83 = as_strided_default_84 = rotary_embedding_default_16 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_106 = torch.ops.aten.split_with_sizes.default(mm_60, [4096, 1024, 1024], -1)
        getitem_505: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_106[1];  split_with_sizes_106 = None
        split_with_sizes_107 = torch.ops.aten.split_with_sizes.default(mm_60, [4096, 1024, 1024], -1)
        getitem_509: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_107[2];  split_with_sizes_107 = None
        view_168: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_505, [-1, 8, 128]);  getitem_505 = None
        view_169: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_509, [-1, 8, 128]);  getitem_509 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_45: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg124_1, 1)
        sym_stride_int_46: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg124_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_31: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg124_1, 0, 1)
        sym_storage_offset_default_15: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_31);  select_31 = None

        # No stacktrace found for following nodes
        as_strided_default_81: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg124_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_45, sym_stride_int_46, arg12_1, 1], 0)
        as_strided_default_82: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg124_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_45, sym_stride_int_46, arg12_1, 1], sym_storage_offset_default_15);  sym_stride_int_45 = sym_stride_int_46 = sym_storage_offset_default_15 = None
        reshape_and_cache_flash_default_16 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_168, view_169, as_strided_default_81, as_strided_default_82, arg15_1, 'auto', 1.0, 1.0);  view_168 = view_169 = as_strided_default_81 = as_strided_default_82 = reshape_and_cache_flash_default_16 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_31: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_77: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_31, [0, 1, 2]);  empty_31 = permute_77 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_109 = torch.ops.aten.split_with_sizes.default(mm_60, [4096, 1024, 1024], -1)
        getitem_515: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_109[0];  split_with_sizes_109 = None
        split_with_sizes_110 = torch.ops.aten.split_with_sizes.default(mm_60, [4096, 1024, 1024], -1)
        getitem_519: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_110[1];  split_with_sizes_110 = None
        split_with_sizes_111 = torch.ops.aten.split_with_sizes.default(mm_60, [4096, 1024, 1024], -1);  mm_60 = None
        getitem_523: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_111[2];  split_with_sizes_111 = None
        view_171: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_515, [-1, 32, 128]);  getitem_515 = None
        slice_94: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_171, 0, 0, arg16_1);  view_171 = None
        view_172: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_519, [-1, 8, 128]);  getitem_519 = None
        slice_95: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_172, 0, 0, arg16_1);  view_172 = None
        view_173: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_523, [-1, 8, 128]);  getitem_523 = None
        slice_96: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_173, 0, 0, arg16_1);  view_173 = None
        flash_attn_varlen_func_15: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_94, slice_95, slice_96, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_94 = slice_95 = slice_96 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_175: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_15, [arg0_1, 4096]);  flash_attn_varlen_func_15 = None
        permute_78: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg125_1, [1, 0]);  arg125_1 = None
        mm_61: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_175, permute_78);  view_175 = permute_78 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_33 = torch.ops._C.fused_add_rms_norm.default(mm_61, embedding, arg126_1, 1e-05);  arg126_1 = fused_add_rms_norm_default_33 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_79: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg127_1, [1, 0]);  arg127_1 = None
        mm_62: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_61, permute_79);  mm_61 = permute_79 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_32: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_80: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_32, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_16 = torch.ops._C.silu_and_mul.default(as_strided_default_80, mm_62);  as_strided_default_80 = mm_62 = silu_and_mul_default_16 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_80: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg128_1, [1, 0]);  arg128_1 = None
        mm_63: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_32, permute_80);  empty_32 = permute_80 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_32 = torch.ops._C.fused_add_rms_norm.default(mm_63, embedding, arg129_1, 1e-05);  arg129_1 = fused_add_rms_norm_default_32 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_81: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg130_1, [1, 0]);  arg130_1 = None
        mm_64: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_63, permute_81);  mm_63 = permute_81 = None

        # No stacktrace found for following nodes
        as_strided_default_78: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_64, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_79: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_64, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_15 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_78, as_strided_default_79, 128, arg5_1, True);  as_strided_default_78 = as_strided_default_79 = rotary_embedding_default_15 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_113 = torch.ops.aten.split_with_sizes.default(mm_64, [4096, 1024, 1024], -1)
        getitem_538: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_113[1];  split_with_sizes_113 = None
        split_with_sizes_114 = torch.ops.aten.split_with_sizes.default(mm_64, [4096, 1024, 1024], -1)
        getitem_542: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_114[2];  split_with_sizes_114 = None
        view_179: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_538, [-1, 8, 128]);  getitem_538 = None
        view_180: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_542, [-1, 8, 128]);  getitem_542 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_48: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg131_1, 1)
        sym_stride_int_49: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg131_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_33: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg131_1, 0, 1)
        sym_storage_offset_default_16: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_33);  select_33 = None

        # No stacktrace found for following nodes
        as_strided_default_76: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg131_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_48, sym_stride_int_49, arg12_1, 1], 0)
        as_strided_default_77: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg131_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_48, sym_stride_int_49, arg12_1, 1], sym_storage_offset_default_16);  sym_stride_int_48 = sym_stride_int_49 = sym_storage_offset_default_16 = None
        reshape_and_cache_flash_default_15 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_179, view_180, as_strided_default_76, as_strided_default_77, arg15_1, 'auto', 1.0, 1.0);  view_179 = view_180 = as_strided_default_76 = as_strided_default_77 = reshape_and_cache_flash_default_15 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_33: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_82: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_33, [0, 1, 2]);  empty_33 = permute_82 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_116 = torch.ops.aten.split_with_sizes.default(mm_64, [4096, 1024, 1024], -1)
        getitem_548: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_116[0];  split_with_sizes_116 = None
        split_with_sizes_117 = torch.ops.aten.split_with_sizes.default(mm_64, [4096, 1024, 1024], -1)
        getitem_552: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_117[1];  split_with_sizes_117 = None
        split_with_sizes_118 = torch.ops.aten.split_with_sizes.default(mm_64, [4096, 1024, 1024], -1);  mm_64 = None
        getitem_556: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_118[2];  split_with_sizes_118 = None
        view_182: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_548, [-1, 32, 128]);  getitem_548 = None
        slice_100: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_182, 0, 0, arg16_1);  view_182 = None
        view_183: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_552, [-1, 8, 128]);  getitem_552 = None
        slice_101: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_183, 0, 0, arg16_1);  view_183 = None
        view_184: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_556, [-1, 8, 128]);  getitem_556 = None
        slice_102: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_184, 0, 0, arg16_1);  view_184 = None
        flash_attn_varlen_func_16: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_100, slice_101, slice_102, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_100 = slice_101 = slice_102 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_186: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_16, [arg0_1, 4096]);  flash_attn_varlen_func_16 = None
        permute_83: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg132_1, [1, 0]);  arg132_1 = None
        mm_65: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_186, permute_83);  view_186 = permute_83 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_31 = torch.ops._C.fused_add_rms_norm.default(mm_65, embedding, arg133_1, 1e-05);  arg133_1 = fused_add_rms_norm_default_31 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_84: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg134_1, [1, 0]);  arg134_1 = None
        mm_66: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_65, permute_84);  mm_65 = permute_84 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_34: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_75: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_34, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_15 = torch.ops._C.silu_and_mul.default(as_strided_default_75, mm_66);  as_strided_default_75 = mm_66 = silu_and_mul_default_15 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_85: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg135_1, [1, 0]);  arg135_1 = None
        mm_67: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_34, permute_85);  empty_34 = permute_85 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_30 = torch.ops._C.fused_add_rms_norm.default(mm_67, embedding, arg136_1, 1e-05);  arg136_1 = fused_add_rms_norm_default_30 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_86: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg137_1, [1, 0]);  arg137_1 = None
        mm_68: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_67, permute_86);  mm_67 = permute_86 = None

        # No stacktrace found for following nodes
        as_strided_default_73: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_68, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_74: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_68, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_14 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_73, as_strided_default_74, 128, arg5_1, True);  as_strided_default_73 = as_strided_default_74 = rotary_embedding_default_14 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_120 = torch.ops.aten.split_with_sizes.default(mm_68, [4096, 1024, 1024], -1)
        getitem_571: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_120[1];  split_with_sizes_120 = None
        split_with_sizes_121 = torch.ops.aten.split_with_sizes.default(mm_68, [4096, 1024, 1024], -1)
        getitem_575: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_121[2];  split_with_sizes_121 = None
        view_190: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_571, [-1, 8, 128]);  getitem_571 = None
        view_191: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_575, [-1, 8, 128]);  getitem_575 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_51: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg138_1, 1)
        sym_stride_int_52: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg138_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_35: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg138_1, 0, 1)
        sym_storage_offset_default_17: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_35);  select_35 = None

        # No stacktrace found for following nodes
        as_strided_default_71: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg138_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_51, sym_stride_int_52, arg12_1, 1], 0)
        as_strided_default_72: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg138_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_51, sym_stride_int_52, arg12_1, 1], sym_storage_offset_default_17);  sym_stride_int_51 = sym_stride_int_52 = sym_storage_offset_default_17 = None
        reshape_and_cache_flash_default_14 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_190, view_191, as_strided_default_71, as_strided_default_72, arg15_1, 'auto', 1.0, 1.0);  view_190 = view_191 = as_strided_default_71 = as_strided_default_72 = reshape_and_cache_flash_default_14 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_35: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_87: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_35, [0, 1, 2]);  empty_35 = permute_87 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_123 = torch.ops.aten.split_with_sizes.default(mm_68, [4096, 1024, 1024], -1)
        getitem_581: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_123[0];  split_with_sizes_123 = None
        split_with_sizes_124 = torch.ops.aten.split_with_sizes.default(mm_68, [4096, 1024, 1024], -1)
        getitem_585: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_124[1];  split_with_sizes_124 = None
        split_with_sizes_125 = torch.ops.aten.split_with_sizes.default(mm_68, [4096, 1024, 1024], -1);  mm_68 = None
        getitem_589: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_125[2];  split_with_sizes_125 = None
        view_193: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_581, [-1, 32, 128]);  getitem_581 = None
        slice_106: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_193, 0, 0, arg16_1);  view_193 = None
        view_194: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_585, [-1, 8, 128]);  getitem_585 = None
        slice_107: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_194, 0, 0, arg16_1);  view_194 = None
        view_195: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_589, [-1, 8, 128]);  getitem_589 = None
        slice_108: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_195, 0, 0, arg16_1);  view_195 = None
        flash_attn_varlen_func_17: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_106, slice_107, slice_108, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_106 = slice_107 = slice_108 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_197: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_17, [arg0_1, 4096]);  flash_attn_varlen_func_17 = None
        permute_88: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg139_1, [1, 0]);  arg139_1 = None
        mm_69: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_197, permute_88);  view_197 = permute_88 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_29 = torch.ops._C.fused_add_rms_norm.default(mm_69, embedding, arg140_1, 1e-05);  arg140_1 = fused_add_rms_norm_default_29 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_89: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg141_1, [1, 0]);  arg141_1 = None
        mm_70: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_69, permute_89);  mm_69 = permute_89 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_36: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_70: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_36, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_14 = torch.ops._C.silu_and_mul.default(as_strided_default_70, mm_70);  as_strided_default_70 = mm_70 = silu_and_mul_default_14 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_90: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg142_1, [1, 0]);  arg142_1 = None
        mm_71: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_36, permute_90);  empty_36 = permute_90 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_28 = torch.ops._C.fused_add_rms_norm.default(mm_71, embedding, arg143_1, 1e-05);  arg143_1 = fused_add_rms_norm_default_28 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_91: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg144_1, [1, 0]);  arg144_1 = None
        mm_72: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_71, permute_91);  mm_71 = permute_91 = None

        # No stacktrace found for following nodes
        as_strided_default_68: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_72, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_69: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_72, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_13 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_68, as_strided_default_69, 128, arg5_1, True);  as_strided_default_68 = as_strided_default_69 = rotary_embedding_default_13 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_127 = torch.ops.aten.split_with_sizes.default(mm_72, [4096, 1024, 1024], -1)
        getitem_604: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_127[1];  split_with_sizes_127 = None
        split_with_sizes_128 = torch.ops.aten.split_with_sizes.default(mm_72, [4096, 1024, 1024], -1)
        getitem_608: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_128[2];  split_with_sizes_128 = None
        view_201: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_604, [-1, 8, 128]);  getitem_604 = None
        view_202: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_608, [-1, 8, 128]);  getitem_608 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_54: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg145_1, 1)
        sym_stride_int_55: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg145_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_37: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg145_1, 0, 1)
        sym_storage_offset_default_18: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_37);  select_37 = None

        # No stacktrace found for following nodes
        as_strided_default_66: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg145_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_54, sym_stride_int_55, arg12_1, 1], 0)
        as_strided_default_67: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg145_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_54, sym_stride_int_55, arg12_1, 1], sym_storage_offset_default_18);  sym_stride_int_54 = sym_stride_int_55 = sym_storage_offset_default_18 = None
        reshape_and_cache_flash_default_13 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_201, view_202, as_strided_default_66, as_strided_default_67, arg15_1, 'auto', 1.0, 1.0);  view_201 = view_202 = as_strided_default_66 = as_strided_default_67 = reshape_and_cache_flash_default_13 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_37: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_92: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_37, [0, 1, 2]);  empty_37 = permute_92 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_130 = torch.ops.aten.split_with_sizes.default(mm_72, [4096, 1024, 1024], -1)
        getitem_614: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_130[0];  split_with_sizes_130 = None
        split_with_sizes_131 = torch.ops.aten.split_with_sizes.default(mm_72, [4096, 1024, 1024], -1)
        getitem_618: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_131[1];  split_with_sizes_131 = None
        split_with_sizes_132 = torch.ops.aten.split_with_sizes.default(mm_72, [4096, 1024, 1024], -1);  mm_72 = None
        getitem_622: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_132[2];  split_with_sizes_132 = None
        view_204: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_614, [-1, 32, 128]);  getitem_614 = None
        slice_112: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_204, 0, 0, arg16_1);  view_204 = None
        view_205: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_618, [-1, 8, 128]);  getitem_618 = None
        slice_113: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_205, 0, 0, arg16_1);  view_205 = None
        view_206: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_622, [-1, 8, 128]);  getitem_622 = None
        slice_114: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_206, 0, 0, arg16_1);  view_206 = None
        flash_attn_varlen_func_18: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_112, slice_113, slice_114, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_112 = slice_113 = slice_114 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_208: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_18, [arg0_1, 4096]);  flash_attn_varlen_func_18 = None
        permute_93: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg146_1, [1, 0]);  arg146_1 = None
        mm_73: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_208, permute_93);  view_208 = permute_93 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_27 = torch.ops._C.fused_add_rms_norm.default(mm_73, embedding, arg147_1, 1e-05);  arg147_1 = fused_add_rms_norm_default_27 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_94: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg148_1, [1, 0]);  arg148_1 = None
        mm_74: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_73, permute_94);  mm_73 = permute_94 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_38: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_65: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_38, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_13 = torch.ops._C.silu_and_mul.default(as_strided_default_65, mm_74);  as_strided_default_65 = mm_74 = silu_and_mul_default_13 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_95: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg149_1, [1, 0]);  arg149_1 = None
        mm_75: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_38, permute_95);  empty_38 = permute_95 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_26 = torch.ops._C.fused_add_rms_norm.default(mm_75, embedding, arg150_1, 1e-05);  arg150_1 = fused_add_rms_norm_default_26 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_96: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg151_1, [1, 0]);  arg151_1 = None
        mm_76: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_75, permute_96);  mm_75 = permute_96 = None

        # No stacktrace found for following nodes
        as_strided_default_63: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_76, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_64: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_76, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_12 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_63, as_strided_default_64, 128, arg5_1, True);  as_strided_default_63 = as_strided_default_64 = rotary_embedding_default_12 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_134 = torch.ops.aten.split_with_sizes.default(mm_76, [4096, 1024, 1024], -1)
        getitem_637: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_134[1];  split_with_sizes_134 = None
        split_with_sizes_135 = torch.ops.aten.split_with_sizes.default(mm_76, [4096, 1024, 1024], -1)
        getitem_641: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_135[2];  split_with_sizes_135 = None
        view_212: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_637, [-1, 8, 128]);  getitem_637 = None
        view_213: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_641, [-1, 8, 128]);  getitem_641 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_57: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg152_1, 1)
        sym_stride_int_58: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg152_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_39: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg152_1, 0, 1)
        sym_storage_offset_default_19: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_39);  select_39 = None

        # No stacktrace found for following nodes
        as_strided_default_61: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg152_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_57, sym_stride_int_58, arg12_1, 1], 0)
        as_strided_default_62: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg152_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_57, sym_stride_int_58, arg12_1, 1], sym_storage_offset_default_19);  sym_stride_int_57 = sym_stride_int_58 = sym_storage_offset_default_19 = None
        reshape_and_cache_flash_default_12 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_212, view_213, as_strided_default_61, as_strided_default_62, arg15_1, 'auto', 1.0, 1.0);  view_212 = view_213 = as_strided_default_61 = as_strided_default_62 = reshape_and_cache_flash_default_12 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_39: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_97: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_39, [0, 1, 2]);  empty_39 = permute_97 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_137 = torch.ops.aten.split_with_sizes.default(mm_76, [4096, 1024, 1024], -1)
        getitem_647: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_137[0];  split_with_sizes_137 = None
        split_with_sizes_138 = torch.ops.aten.split_with_sizes.default(mm_76, [4096, 1024, 1024], -1)
        getitem_651: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_138[1];  split_with_sizes_138 = None
        split_with_sizes_139 = torch.ops.aten.split_with_sizes.default(mm_76, [4096, 1024, 1024], -1);  mm_76 = None
        getitem_655: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_139[2];  split_with_sizes_139 = None
        view_215: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_647, [-1, 32, 128]);  getitem_647 = None
        slice_118: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_215, 0, 0, arg16_1);  view_215 = None
        view_216: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_651, [-1, 8, 128]);  getitem_651 = None
        slice_119: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_216, 0, 0, arg16_1);  view_216 = None
        view_217: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_655, [-1, 8, 128]);  getitem_655 = None
        slice_120: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_217, 0, 0, arg16_1);  view_217 = None
        flash_attn_varlen_func_19: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_118, slice_119, slice_120, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_118 = slice_119 = slice_120 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_219: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_19, [arg0_1, 4096]);  flash_attn_varlen_func_19 = None
        permute_98: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg153_1, [1, 0]);  arg153_1 = None
        mm_77: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_219, permute_98);  view_219 = permute_98 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_25 = torch.ops._C.fused_add_rms_norm.default(mm_77, embedding, arg154_1, 1e-05);  arg154_1 = fused_add_rms_norm_default_25 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_99: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg155_1, [1, 0]);  arg155_1 = None
        mm_78: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_77, permute_99);  mm_77 = permute_99 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_40: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_60: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_40, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_12 = torch.ops._C.silu_and_mul.default(as_strided_default_60, mm_78);  as_strided_default_60 = mm_78 = silu_and_mul_default_12 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_100: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg156_1, [1, 0]);  arg156_1 = None
        mm_79: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_40, permute_100);  empty_40 = permute_100 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_24 = torch.ops._C.fused_add_rms_norm.default(mm_79, embedding, arg157_1, 1e-05);  arg157_1 = fused_add_rms_norm_default_24 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_101: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg158_1, [1, 0]);  arg158_1 = None
        mm_80: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_79, permute_101);  mm_79 = permute_101 = None

        # No stacktrace found for following nodes
        as_strided_default_58: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_80, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_59: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_80, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_11 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_58, as_strided_default_59, 128, arg5_1, True);  as_strided_default_58 = as_strided_default_59 = rotary_embedding_default_11 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_141 = torch.ops.aten.split_with_sizes.default(mm_80, [4096, 1024, 1024], -1)
        getitem_670: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_141[1];  split_with_sizes_141 = None
        split_with_sizes_142 = torch.ops.aten.split_with_sizes.default(mm_80, [4096, 1024, 1024], -1)
        getitem_674: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_142[2];  split_with_sizes_142 = None
        view_223: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_670, [-1, 8, 128]);  getitem_670 = None
        view_224: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_674, [-1, 8, 128]);  getitem_674 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_60: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg159_1, 1)
        sym_stride_int_61: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg159_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_41: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg159_1, 0, 1)
        sym_storage_offset_default_20: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_41);  select_41 = None

        # No stacktrace found for following nodes
        as_strided_default_56: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg159_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_60, sym_stride_int_61, arg12_1, 1], 0)
        as_strided_default_57: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg159_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_60, sym_stride_int_61, arg12_1, 1], sym_storage_offset_default_20);  sym_stride_int_60 = sym_stride_int_61 = sym_storage_offset_default_20 = None
        reshape_and_cache_flash_default_11 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_223, view_224, as_strided_default_56, as_strided_default_57, arg15_1, 'auto', 1.0, 1.0);  view_223 = view_224 = as_strided_default_56 = as_strided_default_57 = reshape_and_cache_flash_default_11 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_41: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_102: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_41, [0, 1, 2]);  empty_41 = permute_102 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_144 = torch.ops.aten.split_with_sizes.default(mm_80, [4096, 1024, 1024], -1)
        getitem_680: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_144[0];  split_with_sizes_144 = None
        split_with_sizes_145 = torch.ops.aten.split_with_sizes.default(mm_80, [4096, 1024, 1024], -1)
        getitem_684: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_145[1];  split_with_sizes_145 = None
        split_with_sizes_146 = torch.ops.aten.split_with_sizes.default(mm_80, [4096, 1024, 1024], -1);  mm_80 = None
        getitem_688: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_146[2];  split_with_sizes_146 = None
        view_226: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_680, [-1, 32, 128]);  getitem_680 = None
        slice_124: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_226, 0, 0, arg16_1);  view_226 = None
        view_227: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_684, [-1, 8, 128]);  getitem_684 = None
        slice_125: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_227, 0, 0, arg16_1);  view_227 = None
        view_228: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_688, [-1, 8, 128]);  getitem_688 = None
        slice_126: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_228, 0, 0, arg16_1);  view_228 = None
        flash_attn_varlen_func_20: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_124, slice_125, slice_126, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_124 = slice_125 = slice_126 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_230: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_20, [arg0_1, 4096]);  flash_attn_varlen_func_20 = None
        permute_103: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg160_1, [1, 0]);  arg160_1 = None
        mm_81: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_230, permute_103);  view_230 = permute_103 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_23 = torch.ops._C.fused_add_rms_norm.default(mm_81, embedding, arg161_1, 1e-05);  arg161_1 = fused_add_rms_norm_default_23 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_104: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg162_1, [1, 0]);  arg162_1 = None
        mm_82: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_81, permute_104);  mm_81 = permute_104 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_42: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_55: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_42, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_11 = torch.ops._C.silu_and_mul.default(as_strided_default_55, mm_82);  as_strided_default_55 = mm_82 = silu_and_mul_default_11 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_105: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg163_1, [1, 0]);  arg163_1 = None
        mm_83: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_42, permute_105);  empty_42 = permute_105 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_22 = torch.ops._C.fused_add_rms_norm.default(mm_83, embedding, arg164_1, 1e-05);  arg164_1 = fused_add_rms_norm_default_22 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_106: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg165_1, [1, 0]);  arg165_1 = None
        mm_84: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_83, permute_106);  mm_83 = permute_106 = None

        # No stacktrace found for following nodes
        as_strided_default_53: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_84, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_54: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_84, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_10 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_53, as_strided_default_54, 128, arg5_1, True);  as_strided_default_53 = as_strided_default_54 = rotary_embedding_default_10 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_148 = torch.ops.aten.split_with_sizes.default(mm_84, [4096, 1024, 1024], -1)
        getitem_703: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_148[1];  split_with_sizes_148 = None
        split_with_sizes_149 = torch.ops.aten.split_with_sizes.default(mm_84, [4096, 1024, 1024], -1)
        getitem_707: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_149[2];  split_with_sizes_149 = None
        view_234: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_703, [-1, 8, 128]);  getitem_703 = None
        view_235: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_707, [-1, 8, 128]);  getitem_707 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_63: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg166_1, 1)
        sym_stride_int_64: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg166_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_43: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg166_1, 0, 1)
        sym_storage_offset_default_21: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_43);  select_43 = None

        # No stacktrace found for following nodes
        as_strided_default_51: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg166_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_63, sym_stride_int_64, arg12_1, 1], 0)
        as_strided_default_52: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg166_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_63, sym_stride_int_64, arg12_1, 1], sym_storage_offset_default_21);  sym_stride_int_63 = sym_stride_int_64 = sym_storage_offset_default_21 = None
        reshape_and_cache_flash_default_10 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_234, view_235, as_strided_default_51, as_strided_default_52, arg15_1, 'auto', 1.0, 1.0);  view_234 = view_235 = as_strided_default_51 = as_strided_default_52 = reshape_and_cache_flash_default_10 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_43: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_107: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_43, [0, 1, 2]);  empty_43 = permute_107 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_151 = torch.ops.aten.split_with_sizes.default(mm_84, [4096, 1024, 1024], -1)
        getitem_713: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_151[0];  split_with_sizes_151 = None
        split_with_sizes_152 = torch.ops.aten.split_with_sizes.default(mm_84, [4096, 1024, 1024], -1)
        getitem_717: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_152[1];  split_with_sizes_152 = None
        split_with_sizes_153 = torch.ops.aten.split_with_sizes.default(mm_84, [4096, 1024, 1024], -1);  mm_84 = None
        getitem_721: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_153[2];  split_with_sizes_153 = None
        view_237: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_713, [-1, 32, 128]);  getitem_713 = None
        slice_130: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_237, 0, 0, arg16_1);  view_237 = None
        view_238: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_717, [-1, 8, 128]);  getitem_717 = None
        slice_131: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_238, 0, 0, arg16_1);  view_238 = None
        view_239: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_721, [-1, 8, 128]);  getitem_721 = None
        slice_132: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_239, 0, 0, arg16_1);  view_239 = None
        flash_attn_varlen_func_21: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_130, slice_131, slice_132, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_130 = slice_131 = slice_132 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_241: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_21, [arg0_1, 4096]);  flash_attn_varlen_func_21 = None
        permute_108: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg167_1, [1, 0]);  arg167_1 = None
        mm_85: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_241, permute_108);  view_241 = permute_108 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_21 = torch.ops._C.fused_add_rms_norm.default(mm_85, embedding, arg168_1, 1e-05);  arg168_1 = fused_add_rms_norm_default_21 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_109: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg169_1, [1, 0]);  arg169_1 = None
        mm_86: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_85, permute_109);  mm_85 = permute_109 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_44: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_50: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_44, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_10 = torch.ops._C.silu_and_mul.default(as_strided_default_50, mm_86);  as_strided_default_50 = mm_86 = silu_and_mul_default_10 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_110: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg170_1, [1, 0]);  arg170_1 = None
        mm_87: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_44, permute_110);  empty_44 = permute_110 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_20 = torch.ops._C.fused_add_rms_norm.default(mm_87, embedding, arg171_1, 1e-05);  arg171_1 = fused_add_rms_norm_default_20 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_111: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg172_1, [1, 0]);  arg172_1 = None
        mm_88: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_87, permute_111);  mm_87 = permute_111 = None

        # No stacktrace found for following nodes
        as_strided_default_48: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_88, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_49: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_88, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_9 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_48, as_strided_default_49, 128, arg5_1, True);  as_strided_default_48 = as_strided_default_49 = rotary_embedding_default_9 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_155 = torch.ops.aten.split_with_sizes.default(mm_88, [4096, 1024, 1024], -1)
        getitem_736: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_155[1];  split_with_sizes_155 = None
        split_with_sizes_156 = torch.ops.aten.split_with_sizes.default(mm_88, [4096, 1024, 1024], -1)
        getitem_740: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_156[2];  split_with_sizes_156 = None
        view_245: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_736, [-1, 8, 128]);  getitem_736 = None
        view_246: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_740, [-1, 8, 128]);  getitem_740 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_66: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg173_1, 1)
        sym_stride_int_67: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg173_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_45: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg173_1, 0, 1)
        sym_storage_offset_default_22: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_45);  select_45 = None

        # No stacktrace found for following nodes
        as_strided_default_46: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg173_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_66, sym_stride_int_67, arg12_1, 1], 0)
        as_strided_default_47: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg173_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_66, sym_stride_int_67, arg12_1, 1], sym_storage_offset_default_22);  sym_stride_int_66 = sym_stride_int_67 = sym_storage_offset_default_22 = None
        reshape_and_cache_flash_default_9 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_245, view_246, as_strided_default_46, as_strided_default_47, arg15_1, 'auto', 1.0, 1.0);  view_245 = view_246 = as_strided_default_46 = as_strided_default_47 = reshape_and_cache_flash_default_9 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_45: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_112: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_45, [0, 1, 2]);  empty_45 = permute_112 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_158 = torch.ops.aten.split_with_sizes.default(mm_88, [4096, 1024, 1024], -1)
        getitem_746: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_158[0];  split_with_sizes_158 = None
        split_with_sizes_159 = torch.ops.aten.split_with_sizes.default(mm_88, [4096, 1024, 1024], -1)
        getitem_750: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_159[1];  split_with_sizes_159 = None
        split_with_sizes_160 = torch.ops.aten.split_with_sizes.default(mm_88, [4096, 1024, 1024], -1);  mm_88 = None
        getitem_754: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_160[2];  split_with_sizes_160 = None
        view_248: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_746, [-1, 32, 128]);  getitem_746 = None
        slice_136: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_248, 0, 0, arg16_1);  view_248 = None
        view_249: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_750, [-1, 8, 128]);  getitem_750 = None
        slice_137: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_249, 0, 0, arg16_1);  view_249 = None
        view_250: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_754, [-1, 8, 128]);  getitem_754 = None
        slice_138: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_250, 0, 0, arg16_1);  view_250 = None
        flash_attn_varlen_func_22: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_136, slice_137, slice_138, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_136 = slice_137 = slice_138 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_252: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_22, [arg0_1, 4096]);  flash_attn_varlen_func_22 = None
        permute_113: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg174_1, [1, 0]);  arg174_1 = None
        mm_89: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_252, permute_113);  view_252 = permute_113 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_19 = torch.ops._C.fused_add_rms_norm.default(mm_89, embedding, arg175_1, 1e-05);  arg175_1 = fused_add_rms_norm_default_19 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_114: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg176_1, [1, 0]);  arg176_1 = None
        mm_90: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_89, permute_114);  mm_89 = permute_114 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_46: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_45: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_46, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_9 = torch.ops._C.silu_and_mul.default(as_strided_default_45, mm_90);  as_strided_default_45 = mm_90 = silu_and_mul_default_9 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_115: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg177_1, [1, 0]);  arg177_1 = None
        mm_91: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_46, permute_115);  empty_46 = permute_115 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_18 = torch.ops._C.fused_add_rms_norm.default(mm_91, embedding, arg178_1, 1e-05);  arg178_1 = fused_add_rms_norm_default_18 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_116: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg179_1, [1, 0]);  arg179_1 = None
        mm_92: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_91, permute_116);  mm_91 = permute_116 = None

        # No stacktrace found for following nodes
        as_strided_default_43: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_92, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_44: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_92, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_8 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_43, as_strided_default_44, 128, arg5_1, True);  as_strided_default_43 = as_strided_default_44 = rotary_embedding_default_8 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_162 = torch.ops.aten.split_with_sizes.default(mm_92, [4096, 1024, 1024], -1)
        getitem_769: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_162[1];  split_with_sizes_162 = None
        split_with_sizes_163 = torch.ops.aten.split_with_sizes.default(mm_92, [4096, 1024, 1024], -1)
        getitem_773: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_163[2];  split_with_sizes_163 = None
        view_256: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_769, [-1, 8, 128]);  getitem_769 = None
        view_257: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_773, [-1, 8, 128]);  getitem_773 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_69: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg180_1, 1)
        sym_stride_int_70: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg180_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_47: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg180_1, 0, 1)
        sym_storage_offset_default_23: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_47);  select_47 = None

        # No stacktrace found for following nodes
        as_strided_default_41: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg180_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_69, sym_stride_int_70, arg12_1, 1], 0)
        as_strided_default_42: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg180_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_69, sym_stride_int_70, arg12_1, 1], sym_storage_offset_default_23);  sym_stride_int_69 = sym_stride_int_70 = sym_storage_offset_default_23 = None
        reshape_and_cache_flash_default_8 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_256, view_257, as_strided_default_41, as_strided_default_42, arg15_1, 'auto', 1.0, 1.0);  view_256 = view_257 = as_strided_default_41 = as_strided_default_42 = reshape_and_cache_flash_default_8 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_47: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_117: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_47, [0, 1, 2]);  empty_47 = permute_117 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_165 = torch.ops.aten.split_with_sizes.default(mm_92, [4096, 1024, 1024], -1)
        getitem_779: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_165[0];  split_with_sizes_165 = None
        split_with_sizes_166 = torch.ops.aten.split_with_sizes.default(mm_92, [4096, 1024, 1024], -1)
        getitem_783: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_166[1];  split_with_sizes_166 = None
        split_with_sizes_167 = torch.ops.aten.split_with_sizes.default(mm_92, [4096, 1024, 1024], -1);  mm_92 = None
        getitem_787: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_167[2];  split_with_sizes_167 = None
        view_259: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_779, [-1, 32, 128]);  getitem_779 = None
        slice_142: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_259, 0, 0, arg16_1);  view_259 = None
        view_260: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_783, [-1, 8, 128]);  getitem_783 = None
        slice_143: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_260, 0, 0, arg16_1);  view_260 = None
        view_261: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_787, [-1, 8, 128]);  getitem_787 = None
        slice_144: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_261, 0, 0, arg16_1);  view_261 = None
        flash_attn_varlen_func_23: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_142, slice_143, slice_144, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_142 = slice_143 = slice_144 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_263: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_23, [arg0_1, 4096]);  flash_attn_varlen_func_23 = None
        permute_118: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg181_1, [1, 0]);  arg181_1 = None
        mm_93: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_263, permute_118);  view_263 = permute_118 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_17 = torch.ops._C.fused_add_rms_norm.default(mm_93, embedding, arg182_1, 1e-05);  arg182_1 = fused_add_rms_norm_default_17 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_119: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg183_1, [1, 0]);  arg183_1 = None
        mm_94: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_93, permute_119);  mm_93 = permute_119 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_48: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_40: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_48, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_8 = torch.ops._C.silu_and_mul.default(as_strided_default_40, mm_94);  as_strided_default_40 = mm_94 = silu_and_mul_default_8 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_120: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg184_1, [1, 0]);  arg184_1 = None
        mm_95: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_48, permute_120);  empty_48 = permute_120 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_16 = torch.ops._C.fused_add_rms_norm.default(mm_95, embedding, arg185_1, 1e-05);  arg185_1 = fused_add_rms_norm_default_16 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_121: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg186_1, [1, 0]);  arg186_1 = None
        mm_96: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_95, permute_121);  mm_95 = permute_121 = None

        # No stacktrace found for following nodes
        as_strided_default_38: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_96, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_39: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_96, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_7 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_38, as_strided_default_39, 128, arg5_1, True);  as_strided_default_38 = as_strided_default_39 = rotary_embedding_default_7 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_169 = torch.ops.aten.split_with_sizes.default(mm_96, [4096, 1024, 1024], -1)
        getitem_802: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_169[1];  split_with_sizes_169 = None
        split_with_sizes_170 = torch.ops.aten.split_with_sizes.default(mm_96, [4096, 1024, 1024], -1)
        getitem_806: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_170[2];  split_with_sizes_170 = None
        view_267: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_802, [-1, 8, 128]);  getitem_802 = None
        view_268: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_806, [-1, 8, 128]);  getitem_806 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_72: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg187_1, 1)
        sym_stride_int_73: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg187_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_49: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg187_1, 0, 1)
        sym_storage_offset_default_24: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_49);  select_49 = None

        # No stacktrace found for following nodes
        as_strided_default_36: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg187_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_72, sym_stride_int_73, arg12_1, 1], 0)
        as_strided_default_37: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg187_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_72, sym_stride_int_73, arg12_1, 1], sym_storage_offset_default_24);  sym_stride_int_72 = sym_stride_int_73 = sym_storage_offset_default_24 = None
        reshape_and_cache_flash_default_7 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_267, view_268, as_strided_default_36, as_strided_default_37, arg15_1, 'auto', 1.0, 1.0);  view_267 = view_268 = as_strided_default_36 = as_strided_default_37 = reshape_and_cache_flash_default_7 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_49: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_122: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_49, [0, 1, 2]);  empty_49 = permute_122 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_172 = torch.ops.aten.split_with_sizes.default(mm_96, [4096, 1024, 1024], -1)
        getitem_812: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_172[0];  split_with_sizes_172 = None
        split_with_sizes_173 = torch.ops.aten.split_with_sizes.default(mm_96, [4096, 1024, 1024], -1)
        getitem_816: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_173[1];  split_with_sizes_173 = None
        split_with_sizes_174 = torch.ops.aten.split_with_sizes.default(mm_96, [4096, 1024, 1024], -1);  mm_96 = None
        getitem_820: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_174[2];  split_with_sizes_174 = None
        view_270: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_812, [-1, 32, 128]);  getitem_812 = None
        slice_148: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_270, 0, 0, arg16_1);  view_270 = None
        view_271: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_816, [-1, 8, 128]);  getitem_816 = None
        slice_149: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_271, 0, 0, arg16_1);  view_271 = None
        view_272: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_820, [-1, 8, 128]);  getitem_820 = None
        slice_150: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_272, 0, 0, arg16_1);  view_272 = None
        flash_attn_varlen_func_24: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_148, slice_149, slice_150, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_148 = slice_149 = slice_150 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_274: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_24, [arg0_1, 4096]);  flash_attn_varlen_func_24 = None
        permute_123: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg188_1, [1, 0]);  arg188_1 = None
        mm_97: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_274, permute_123);  view_274 = permute_123 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_15 = torch.ops._C.fused_add_rms_norm.default(mm_97, embedding, arg189_1, 1e-05);  arg189_1 = fused_add_rms_norm_default_15 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_124: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg190_1, [1, 0]);  arg190_1 = None
        mm_98: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_97, permute_124);  mm_97 = permute_124 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_50: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_35: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_50, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_7 = torch.ops._C.silu_and_mul.default(as_strided_default_35, mm_98);  as_strided_default_35 = mm_98 = silu_and_mul_default_7 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_125: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg191_1, [1, 0]);  arg191_1 = None
        mm_99: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_50, permute_125);  empty_50 = permute_125 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_14 = torch.ops._C.fused_add_rms_norm.default(mm_99, embedding, arg192_1, 1e-05);  arg192_1 = fused_add_rms_norm_default_14 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_126: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg193_1, [1, 0]);  arg193_1 = None
        mm_100: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_99, permute_126);  mm_99 = permute_126 = None

        # No stacktrace found for following nodes
        as_strided_default_33: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_100, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_34: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_100, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_6 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_33, as_strided_default_34, 128, arg5_1, True);  as_strided_default_33 = as_strided_default_34 = rotary_embedding_default_6 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_176 = torch.ops.aten.split_with_sizes.default(mm_100, [4096, 1024, 1024], -1)
        getitem_835: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_176[1];  split_with_sizes_176 = None
        split_with_sizes_177 = torch.ops.aten.split_with_sizes.default(mm_100, [4096, 1024, 1024], -1)
        getitem_839: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_177[2];  split_with_sizes_177 = None
        view_278: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_835, [-1, 8, 128]);  getitem_835 = None
        view_279: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_839, [-1, 8, 128]);  getitem_839 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_75: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg194_1, 1)
        sym_stride_int_76: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg194_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_51: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg194_1, 0, 1)
        sym_storage_offset_default_25: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_51);  select_51 = None

        # No stacktrace found for following nodes
        as_strided_default_31: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg194_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_75, sym_stride_int_76, arg12_1, 1], 0)
        as_strided_default_32: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg194_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_75, sym_stride_int_76, arg12_1, 1], sym_storage_offset_default_25);  sym_stride_int_75 = sym_stride_int_76 = sym_storage_offset_default_25 = None
        reshape_and_cache_flash_default_6 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_278, view_279, as_strided_default_31, as_strided_default_32, arg15_1, 'auto', 1.0, 1.0);  view_278 = view_279 = as_strided_default_31 = as_strided_default_32 = reshape_and_cache_flash_default_6 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_51: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_127: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_51, [0, 1, 2]);  empty_51 = permute_127 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_179 = torch.ops.aten.split_with_sizes.default(mm_100, [4096, 1024, 1024], -1)
        getitem_845: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_179[0];  split_with_sizes_179 = None
        split_with_sizes_180 = torch.ops.aten.split_with_sizes.default(mm_100, [4096, 1024, 1024], -1)
        getitem_849: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_180[1];  split_with_sizes_180 = None
        split_with_sizes_181 = torch.ops.aten.split_with_sizes.default(mm_100, [4096, 1024, 1024], -1);  mm_100 = None
        getitem_853: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_181[2];  split_with_sizes_181 = None
        view_281: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_845, [-1, 32, 128]);  getitem_845 = None
        slice_154: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_281, 0, 0, arg16_1);  view_281 = None
        view_282: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_849, [-1, 8, 128]);  getitem_849 = None
        slice_155: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_282, 0, 0, arg16_1);  view_282 = None
        view_283: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_853, [-1, 8, 128]);  getitem_853 = None
        slice_156: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_283, 0, 0, arg16_1);  view_283 = None
        flash_attn_varlen_func_25: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_154, slice_155, slice_156, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_154 = slice_155 = slice_156 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_285: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_25, [arg0_1, 4096]);  flash_attn_varlen_func_25 = None
        permute_128: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg195_1, [1, 0]);  arg195_1 = None
        mm_101: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_285, permute_128);  view_285 = permute_128 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_13 = torch.ops._C.fused_add_rms_norm.default(mm_101, embedding, arg196_1, 1e-05);  arg196_1 = fused_add_rms_norm_default_13 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_129: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg197_1, [1, 0]);  arg197_1 = None
        mm_102: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_101, permute_129);  mm_101 = permute_129 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_52: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_30: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_52, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_6 = torch.ops._C.silu_and_mul.default(as_strided_default_30, mm_102);  as_strided_default_30 = mm_102 = silu_and_mul_default_6 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_130: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg198_1, [1, 0]);  arg198_1 = None
        mm_103: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_52, permute_130);  empty_52 = permute_130 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_12 = torch.ops._C.fused_add_rms_norm.default(mm_103, embedding, arg199_1, 1e-05);  arg199_1 = fused_add_rms_norm_default_12 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_131: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg200_1, [1, 0]);  arg200_1 = None
        mm_104: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_103, permute_131);  mm_103 = permute_131 = None

        # No stacktrace found for following nodes
        as_strided_default_28: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_104, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_29: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_104, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_5 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_28, as_strided_default_29, 128, arg5_1, True);  as_strided_default_28 = as_strided_default_29 = rotary_embedding_default_5 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_183 = torch.ops.aten.split_with_sizes.default(mm_104, [4096, 1024, 1024], -1)
        getitem_868: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_183[1];  split_with_sizes_183 = None
        split_with_sizes_184 = torch.ops.aten.split_with_sizes.default(mm_104, [4096, 1024, 1024], -1)
        getitem_872: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_184[2];  split_with_sizes_184 = None
        view_289: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_868, [-1, 8, 128]);  getitem_868 = None
        view_290: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_872, [-1, 8, 128]);  getitem_872 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_78: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg201_1, 1)
        sym_stride_int_79: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg201_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_53: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg201_1, 0, 1)
        sym_storage_offset_default_26: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_53);  select_53 = None

        # No stacktrace found for following nodes
        as_strided_default_26: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg201_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_78, sym_stride_int_79, arg12_1, 1], 0)
        as_strided_default_27: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg201_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_78, sym_stride_int_79, arg12_1, 1], sym_storage_offset_default_26);  sym_stride_int_78 = sym_stride_int_79 = sym_storage_offset_default_26 = None
        reshape_and_cache_flash_default_5 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_289, view_290, as_strided_default_26, as_strided_default_27, arg15_1, 'auto', 1.0, 1.0);  view_289 = view_290 = as_strided_default_26 = as_strided_default_27 = reshape_and_cache_flash_default_5 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_53: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_132: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_53, [0, 1, 2]);  empty_53 = permute_132 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_186 = torch.ops.aten.split_with_sizes.default(mm_104, [4096, 1024, 1024], -1)
        getitem_878: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_186[0];  split_with_sizes_186 = None
        split_with_sizes_187 = torch.ops.aten.split_with_sizes.default(mm_104, [4096, 1024, 1024], -1)
        getitem_882: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_187[1];  split_with_sizes_187 = None
        split_with_sizes_188 = torch.ops.aten.split_with_sizes.default(mm_104, [4096, 1024, 1024], -1);  mm_104 = None
        getitem_886: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_188[2];  split_with_sizes_188 = None
        view_292: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_878, [-1, 32, 128]);  getitem_878 = None
        slice_160: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_292, 0, 0, arg16_1);  view_292 = None
        view_293: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_882, [-1, 8, 128]);  getitem_882 = None
        slice_161: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_293, 0, 0, arg16_1);  view_293 = None
        view_294: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_886, [-1, 8, 128]);  getitem_886 = None
        slice_162: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_294, 0, 0, arg16_1);  view_294 = None
        flash_attn_varlen_func_26: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_160, slice_161, slice_162, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_160 = slice_161 = slice_162 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_296: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_26, [arg0_1, 4096]);  flash_attn_varlen_func_26 = None
        permute_133: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg202_1, [1, 0]);  arg202_1 = None
        mm_105: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_296, permute_133);  view_296 = permute_133 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_11 = torch.ops._C.fused_add_rms_norm.default(mm_105, embedding, arg203_1, 1e-05);  arg203_1 = fused_add_rms_norm_default_11 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_134: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg204_1, [1, 0]);  arg204_1 = None
        mm_106: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_105, permute_134);  mm_105 = permute_134 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_54: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_25: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_54, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_5 = torch.ops._C.silu_and_mul.default(as_strided_default_25, mm_106);  as_strided_default_25 = mm_106 = silu_and_mul_default_5 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_135: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg205_1, [1, 0]);  arg205_1 = None
        mm_107: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_54, permute_135);  empty_54 = permute_135 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_10 = torch.ops._C.fused_add_rms_norm.default(mm_107, embedding, arg206_1, 1e-05);  arg206_1 = fused_add_rms_norm_default_10 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_136: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg207_1, [1, 0]);  arg207_1 = None
        mm_108: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_107, permute_136);  mm_107 = permute_136 = None

        # No stacktrace found for following nodes
        as_strided_default_23: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_108, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_24: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_108, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_4 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_23, as_strided_default_24, 128, arg5_1, True);  as_strided_default_23 = as_strided_default_24 = rotary_embedding_default_4 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_190 = torch.ops.aten.split_with_sizes.default(mm_108, [4096, 1024, 1024], -1)
        getitem_901: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_190[1];  split_with_sizes_190 = None
        split_with_sizes_191 = torch.ops.aten.split_with_sizes.default(mm_108, [4096, 1024, 1024], -1)
        getitem_905: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_191[2];  split_with_sizes_191 = None
        view_300: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_901, [-1, 8, 128]);  getitem_901 = None
        view_301: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_905, [-1, 8, 128]);  getitem_905 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_81: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg208_1, 1)
        sym_stride_int_82: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg208_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_55: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg208_1, 0, 1)
        sym_storage_offset_default_27: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_55);  select_55 = None

        # No stacktrace found for following nodes
        as_strided_default_21: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg208_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_81, sym_stride_int_82, arg12_1, 1], 0)
        as_strided_default_22: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg208_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_81, sym_stride_int_82, arg12_1, 1], sym_storage_offset_default_27);  sym_stride_int_81 = sym_stride_int_82 = sym_storage_offset_default_27 = None
        reshape_and_cache_flash_default_4 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_300, view_301, as_strided_default_21, as_strided_default_22, arg15_1, 'auto', 1.0, 1.0);  view_300 = view_301 = as_strided_default_21 = as_strided_default_22 = reshape_and_cache_flash_default_4 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_55: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_137: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_55, [0, 1, 2]);  empty_55 = permute_137 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_193 = torch.ops.aten.split_with_sizes.default(mm_108, [4096, 1024, 1024], -1)
        getitem_911: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_193[0];  split_with_sizes_193 = None
        split_with_sizes_194 = torch.ops.aten.split_with_sizes.default(mm_108, [4096, 1024, 1024], -1)
        getitem_915: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_194[1];  split_with_sizes_194 = None
        split_with_sizes_195 = torch.ops.aten.split_with_sizes.default(mm_108, [4096, 1024, 1024], -1);  mm_108 = None
        getitem_919: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_195[2];  split_with_sizes_195 = None
        view_303: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_911, [-1, 32, 128]);  getitem_911 = None
        slice_166: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_303, 0, 0, arg16_1);  view_303 = None
        view_304: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_915, [-1, 8, 128]);  getitem_915 = None
        slice_167: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_304, 0, 0, arg16_1);  view_304 = None
        view_305: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_919, [-1, 8, 128]);  getitem_919 = None
        slice_168: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_305, 0, 0, arg16_1);  view_305 = None
        flash_attn_varlen_func_27: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_166, slice_167, slice_168, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_166 = slice_167 = slice_168 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_307: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_27, [arg0_1, 4096]);  flash_attn_varlen_func_27 = None
        permute_138: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg209_1, [1, 0]);  arg209_1 = None
        mm_109: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_307, permute_138);  view_307 = permute_138 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_9 = torch.ops._C.fused_add_rms_norm.default(mm_109, embedding, arg210_1, 1e-05);  arg210_1 = fused_add_rms_norm_default_9 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_139: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg211_1, [1, 0]);  arg211_1 = None
        mm_110: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_109, permute_139);  mm_109 = permute_139 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_56: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_20: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_56, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_4 = torch.ops._C.silu_and_mul.default(as_strided_default_20, mm_110);  as_strided_default_20 = mm_110 = silu_and_mul_default_4 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_140: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg212_1, [1, 0]);  arg212_1 = None
        mm_111: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_56, permute_140);  empty_56 = permute_140 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_8 = torch.ops._C.fused_add_rms_norm.default(mm_111, embedding, arg213_1, 1e-05);  arg213_1 = fused_add_rms_norm_default_8 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_141: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg214_1, [1, 0]);  arg214_1 = None
        mm_112: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_111, permute_141);  mm_111 = permute_141 = None

        # No stacktrace found for following nodes
        as_strided_default_18: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_112, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_19: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_112, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_3 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_18, as_strided_default_19, 128, arg5_1, True);  as_strided_default_18 = as_strided_default_19 = rotary_embedding_default_3 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_197 = torch.ops.aten.split_with_sizes.default(mm_112, [4096, 1024, 1024], -1)
        getitem_934: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_197[1];  split_with_sizes_197 = None
        split_with_sizes_198 = torch.ops.aten.split_with_sizes.default(mm_112, [4096, 1024, 1024], -1)
        getitem_938: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_198[2];  split_with_sizes_198 = None
        view_311: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_934, [-1, 8, 128]);  getitem_934 = None
        view_312: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_938, [-1, 8, 128]);  getitem_938 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_84: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg215_1, 1)
        sym_stride_int_85: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg215_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_57: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg215_1, 0, 1)
        sym_storage_offset_default_28: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_57);  select_57 = None

        # No stacktrace found for following nodes
        as_strided_default_16: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg215_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_84, sym_stride_int_85, arg12_1, 1], 0)
        as_strided_default_17: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg215_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_84, sym_stride_int_85, arg12_1, 1], sym_storage_offset_default_28);  sym_stride_int_84 = sym_stride_int_85 = sym_storage_offset_default_28 = None
        reshape_and_cache_flash_default_3 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_311, view_312, as_strided_default_16, as_strided_default_17, arg15_1, 'auto', 1.0, 1.0);  view_311 = view_312 = as_strided_default_16 = as_strided_default_17 = reshape_and_cache_flash_default_3 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_57: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_142: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_57, [0, 1, 2]);  empty_57 = permute_142 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_200 = torch.ops.aten.split_with_sizes.default(mm_112, [4096, 1024, 1024], -1)
        getitem_944: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_200[0];  split_with_sizes_200 = None
        split_with_sizes_201 = torch.ops.aten.split_with_sizes.default(mm_112, [4096, 1024, 1024], -1)
        getitem_948: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_201[1];  split_with_sizes_201 = None
        split_with_sizes_202 = torch.ops.aten.split_with_sizes.default(mm_112, [4096, 1024, 1024], -1);  mm_112 = None
        getitem_952: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_202[2];  split_with_sizes_202 = None
        view_314: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_944, [-1, 32, 128]);  getitem_944 = None
        slice_172: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_314, 0, 0, arg16_1);  view_314 = None
        view_315: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_948, [-1, 8, 128]);  getitem_948 = None
        slice_173: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_315, 0, 0, arg16_1);  view_315 = None
        view_316: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_952, [-1, 8, 128]);  getitem_952 = None
        slice_174: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_316, 0, 0, arg16_1);  view_316 = None
        flash_attn_varlen_func_28: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_172, slice_173, slice_174, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_172 = slice_173 = slice_174 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_318: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_28, [arg0_1, 4096]);  flash_attn_varlen_func_28 = None
        permute_143: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg216_1, [1, 0]);  arg216_1 = None
        mm_113: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_318, permute_143);  view_318 = permute_143 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_7 = torch.ops._C.fused_add_rms_norm.default(mm_113, embedding, arg217_1, 1e-05);  arg217_1 = fused_add_rms_norm_default_7 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_144: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg218_1, [1, 0]);  arg218_1 = None
        mm_114: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_113, permute_144);  mm_113 = permute_144 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_58: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_15: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_58, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_3 = torch.ops._C.silu_and_mul.default(as_strided_default_15, mm_114);  as_strided_default_15 = mm_114 = silu_and_mul_default_3 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_145: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg219_1, [1, 0]);  arg219_1 = None
        mm_115: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_58, permute_145);  empty_58 = permute_145 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_6 = torch.ops._C.fused_add_rms_norm.default(mm_115, embedding, arg220_1, 1e-05);  arg220_1 = fused_add_rms_norm_default_6 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_146: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg221_1, [1, 0]);  arg221_1 = None
        mm_116: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_115, permute_146);  mm_115 = permute_146 = None

        # No stacktrace found for following nodes
        as_strided_default_13: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_116, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_14: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_116, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_2 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_13, as_strided_default_14, 128, arg5_1, True);  as_strided_default_13 = as_strided_default_14 = rotary_embedding_default_2 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_204 = torch.ops.aten.split_with_sizes.default(mm_116, [4096, 1024, 1024], -1)
        getitem_967: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_204[1];  split_with_sizes_204 = None
        split_with_sizes_205 = torch.ops.aten.split_with_sizes.default(mm_116, [4096, 1024, 1024], -1)
        getitem_971: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_205[2];  split_with_sizes_205 = None
        view_322: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_967, [-1, 8, 128]);  getitem_967 = None
        view_323: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_971, [-1, 8, 128]);  getitem_971 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_87: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg222_1, 1)
        sym_stride_int_88: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg222_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_59: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg222_1, 0, 1)
        sym_storage_offset_default_29: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_59);  select_59 = None

        # No stacktrace found for following nodes
        as_strided_default_11: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg222_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_87, sym_stride_int_88, arg12_1, 1], 0)
        as_strided_default_12: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg222_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_87, sym_stride_int_88, arg12_1, 1], sym_storage_offset_default_29);  sym_stride_int_87 = sym_stride_int_88 = sym_storage_offset_default_29 = None
        reshape_and_cache_flash_default_2 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_322, view_323, as_strided_default_11, as_strided_default_12, arg15_1, 'auto', 1.0, 1.0);  view_322 = view_323 = as_strided_default_11 = as_strided_default_12 = reshape_and_cache_flash_default_2 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_59: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_147: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_59, [0, 1, 2]);  empty_59 = permute_147 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_207 = torch.ops.aten.split_with_sizes.default(mm_116, [4096, 1024, 1024], -1)
        getitem_977: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_207[0];  split_with_sizes_207 = None
        split_with_sizes_208 = torch.ops.aten.split_with_sizes.default(mm_116, [4096, 1024, 1024], -1)
        getitem_981: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_208[1];  split_with_sizes_208 = None
        split_with_sizes_209 = torch.ops.aten.split_with_sizes.default(mm_116, [4096, 1024, 1024], -1);  mm_116 = None
        getitem_985: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_209[2];  split_with_sizes_209 = None
        view_325: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_977, [-1, 32, 128]);  getitem_977 = None
        slice_178: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_325, 0, 0, arg16_1);  view_325 = None
        view_326: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_981, [-1, 8, 128]);  getitem_981 = None
        slice_179: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_326, 0, 0, arg16_1);  view_326 = None
        view_327: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_985, [-1, 8, 128]);  getitem_985 = None
        slice_180: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_327, 0, 0, arg16_1);  view_327 = None
        flash_attn_varlen_func_29: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_178, slice_179, slice_180, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_178 = slice_179 = slice_180 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_329: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_29, [arg0_1, 4096]);  flash_attn_varlen_func_29 = None
        permute_148: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg223_1, [1, 0]);  arg223_1 = None
        mm_117: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_329, permute_148);  view_329 = permute_148 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_5 = torch.ops._C.fused_add_rms_norm.default(mm_117, embedding, arg224_1, 1e-05);  arg224_1 = fused_add_rms_norm_default_5 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_149: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg225_1, [1, 0]);  arg225_1 = None
        mm_118: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_117, permute_149);  mm_117 = permute_149 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_60: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_10: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_60, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_2 = torch.ops._C.silu_and_mul.default(as_strided_default_10, mm_118);  as_strided_default_10 = mm_118 = silu_and_mul_default_2 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_150: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg226_1, [1, 0]);  arg226_1 = None
        mm_119: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_60, permute_150);  empty_60 = permute_150 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_4 = torch.ops._C.fused_add_rms_norm.default(mm_119, embedding, arg227_1, 1e-05);  arg227_1 = fused_add_rms_norm_default_4 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_151: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg228_1, [1, 0]);  arg228_1 = None
        mm_120: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_119, permute_151);  mm_119 = permute_151 = None

        # No stacktrace found for following nodes
        as_strided_default_8: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_120, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_9: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_120, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default_1 = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_8, as_strided_default_9, 128, arg5_1, True);  as_strided_default_8 = as_strided_default_9 = rotary_embedding_default_1 = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_211 = torch.ops.aten.split_with_sizes.default(mm_120, [4096, 1024, 1024], -1)
        getitem_1000: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_211[1];  split_with_sizes_211 = None
        split_with_sizes_212 = torch.ops.aten.split_with_sizes.default(mm_120, [4096, 1024, 1024], -1)
        getitem_1004: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_212[2];  split_with_sizes_212 = None
        view_333: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1000, [-1, 8, 128]);  getitem_1000 = None
        view_334: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1004, [-1, 8, 128]);  getitem_1004 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_90: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg229_1, 1)
        sym_stride_int_91: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg229_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_61: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg229_1, 0, 1)
        sym_storage_offset_default_30: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_61);  select_61 = None

        # No stacktrace found for following nodes
        as_strided_default_6: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg229_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_90, sym_stride_int_91, arg12_1, 1], 0)
        as_strided_default_7: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg229_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_90, sym_stride_int_91, arg12_1, 1], sym_storage_offset_default_30);  sym_stride_int_90 = sym_stride_int_91 = sym_storage_offset_default_30 = None
        reshape_and_cache_flash_default_1 = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_333, view_334, as_strided_default_6, as_strided_default_7, arg15_1, 'auto', 1.0, 1.0);  view_333 = view_334 = as_strided_default_6 = as_strided_default_7 = reshape_and_cache_flash_default_1 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_61: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_152: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_61, [0, 1, 2]);  empty_61 = permute_152 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_214 = torch.ops.aten.split_with_sizes.default(mm_120, [4096, 1024, 1024], -1)
        getitem_1010: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_214[0];  split_with_sizes_214 = None
        split_with_sizes_215 = torch.ops.aten.split_with_sizes.default(mm_120, [4096, 1024, 1024], -1)
        getitem_1014: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_215[1];  split_with_sizes_215 = None
        split_with_sizes_216 = torch.ops.aten.split_with_sizes.default(mm_120, [4096, 1024, 1024], -1);  mm_120 = None
        getitem_1018: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_216[2];  split_with_sizes_216 = None
        view_336: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1010, [-1, 32, 128]);  getitem_1010 = None
        slice_184: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_336, 0, 0, arg16_1);  view_336 = None
        view_337: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1014, [-1, 8, 128]);  getitem_1014 = None
        slice_185: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_337, 0, 0, arg16_1);  view_337 = None
        view_338: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1018, [-1, 8, 128]);  getitem_1018 = None
        slice_186: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_338, 0, 0, arg16_1);  view_338 = None
        flash_attn_varlen_func_30: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_184, slice_185, slice_186, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_184 = slice_185 = slice_186 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_340: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_30, [arg0_1, 4096]);  flash_attn_varlen_func_30 = None
        permute_153: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg230_1, [1, 0]);  arg230_1 = None
        mm_121: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_340, permute_153);  view_340 = permute_153 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_3 = torch.ops._C.fused_add_rms_norm.default(mm_121, embedding, arg231_1, 1e-05);  arg231_1 = fused_add_rms_norm_default_3 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_154: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg232_1, [1, 0]);  arg232_1 = None
        mm_122: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_121, permute_154);  mm_121 = permute_154 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_62: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default_5: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_62, [arg0_1, 14336], [14336, 1], 0)
        silu_and_mul_default_1 = torch.ops._C.silu_and_mul.default(as_strided_default_5, mm_122);  as_strided_default_5 = mm_122 = silu_and_mul_default_1 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_155: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg233_1, [1, 0]);  arg233_1 = None
        mm_123: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_62, permute_155);  empty_62 = permute_155 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_2 = torch.ops._C.fused_add_rms_norm.default(mm_123, embedding, arg234_1, 1e-05);  arg234_1 = fused_add_rms_norm_default_2 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_156: "bf16[4096, 6144][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg235_1, [1, 0]);  arg235_1 = None
        mm_124: "bf16[s1, 6144][6144, 1]cuda:0" = torch.ops.aten.mm.default(mm_123, permute_156);  mm_123 = permute_156 = None

        # No stacktrace found for following nodes
        as_strided_default_3: "bf16[s1, 4096][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_124, [arg0_1, 4096], [6144, 1], 0)
        as_strided_default_4: "bf16[s1, 1024][6144, 1]cuda:0" = torch.ops.aten.as_strided.default(mm_124, [arg0_1, 1024], [6144, 1], 4096)
        rotary_embedding_default = torch.ops._C.rotary_embedding.default(arg7_1, as_strided_default_3, as_strided_default_4, 128, arg5_1, True);  arg7_1 = as_strided_default_3 = as_strided_default_4 = rotary_embedding_default = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        split_with_sizes_218 = torch.ops.aten.split_with_sizes.default(mm_124, [4096, 1024, 1024], -1)
        getitem_1033: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_218[1];  split_with_sizes_218 = None
        split_with_sizes_219 = torch.ops.aten.split_with_sizes.default(mm_124, [4096, 1024, 1024], -1)
        getitem_1037: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_219[2];  split_with_sizes_219 = None
        view_344: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1033, [-1, 8, 128]);  getitem_1033 = None
        view_345: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1037, [-1, 8, 128]);  getitem_1037 = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:599 in forward, code: key_cache = kv_cache[0]
        sym_stride_int_93: "Sym(s7*s8*s9)" = torch.ops.aten.sym_stride.int(arg236_1, 1)
        sym_stride_int_94: "Sym(s8*s9)" = torch.ops.aten.sym_stride.int(arg236_1, 2)

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:600 in forward, code: value_cache = kv_cache[1]
        select_63: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.select.int(arg236_1, 0, 1)
        sym_storage_offset_default_31: "Sym(s6*s7*s8*s9)" = torch.ops.aten.sym_storage_offset.default(select_63);  select_63 = None

        # No stacktrace found for following nodes
        as_strided_default_1: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg236_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_93, sym_stride_int_94, arg12_1, 1], 0)
        as_strided_default_2: "bf16[s6, s7, s8, s9][s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.as_strided.default(arg236_1, [arg9_1, arg10_1, arg11_1, arg12_1], [sym_stride_int_93, sym_stride_int_94, arg12_1, 1], sym_storage_offset_default_31);  arg9_1 = arg10_1 = arg11_1 = arg12_1 = sym_stride_int_93 = sym_stride_int_94 = sym_storage_offset_default_31 = None
        reshape_and_cache_flash_default = torch.ops._C_cache_ops.reshape_and_cache_flash.default(view_344, view_345, as_strided_default_1, as_strided_default_2, arg15_1, 'auto', 1.0, 1.0);  view_344 = view_345 = as_strided_default_1 = as_strided_default_2 = arg15_1 = reshape_and_cache_flash_default = None

         # File: /home/lsakka/vllm/vllm/attention/backends/flash_attn.py:620 in forward, code: output = torch.empty_like(query)
        empty_63: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 32, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        permute_157: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(empty_63, [0, 1, 2]);  empty_63 = permute_157 = None

         # File: /home/lsakka/pytorch/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
        split_with_sizes_221 = torch.ops.aten.split_with_sizes.default(mm_124, [4096, 1024, 1024], -1)
        getitem_1043: "bf16[s1, 4096][6144, 1]cuda:0" = split_with_sizes_221[0];  split_with_sizes_221 = None
        split_with_sizes_222 = torch.ops.aten.split_with_sizes.default(mm_124, [4096, 1024, 1024], -1)
        getitem_1047: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_222[1];  split_with_sizes_222 = None
        split_with_sizes_223 = torch.ops.aten.split_with_sizes.default(mm_124, [4096, 1024, 1024], -1);  mm_124 = None
        getitem_1051: "bf16[s1, 1024][6144, 1]cuda:0" = split_with_sizes_223[2];  split_with_sizes_223 = None
        view_347: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1043, [-1, 32, 128]);  getitem_1043 = None
        slice_190: "bf16[s1, 32, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_347, 0, 0, arg16_1);  view_347 = None
        view_348: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1047, [-1, 8, 128]);  getitem_1047 = None
        slice_191: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_348, 0, 0, arg16_1);  view_348 = None
        view_349: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1051, [-1, 8, 128]);  getitem_1051 = None
        slice_192: "bf16[s1, 8, 128][6144, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_349, 0, 0, arg16_1);  view_349 = arg16_1 = None
        flash_attn_varlen_func_31: "bf16[s1, 32, 128][4096, 128, 1]cuda:0" = torch.ops.vllm.flash_attn_varlen_func.default(slice_190, slice_191, slice_192, arg19_1, arg19_1, arg17_1, arg17_1, 0.08838834764831845, True, [-1, -1]);  slice_190 = slice_191 = slice_192 = arg19_1 = arg17_1 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        view_351: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(flash_attn_varlen_func_31, [arg0_1, 4096]);  flash_attn_varlen_func_31 = None
        permute_158: "bf16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg237_1, [1, 0]);  arg237_1 = None
        mm_125: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_351, permute_158);  view_351 = permute_158 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default_1 = torch.ops._C.fused_add_rms_norm.default(mm_125, embedding, arg238_1, 1e-05);  arg238_1 = fused_add_rms_norm_default_1 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_159: "bf16[4096, 28672][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg239_1, [1, 0]);  arg239_1 = None
        mm_126: "bf16[s1, 28672][28672, 1]cuda:0" = torch.ops.aten.mm.default(mm_125, permute_159);  mm_125 = permute_159 = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/activation.py:36 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
        empty_64: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 14336], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)

        # No stacktrace found for following nodes
        as_strided_default: "bf16[s1, 14336][14336, 1]cuda:0" = torch.ops.aten.as_strided.default(empty_64, [arg0_1, 14336], [14336, 1], 0);  arg0_1 = None
        silu_and_mul_default = torch.ops._C.silu_and_mul.default(as_strided_default, mm_126);  as_strided_default = mm_126 = silu_and_mul_default = None

         # File: /home/lsakka/vllm/vllm/model_executor/layers/linear.py:129 in apply, code: return F.linear(x, layer.weight, bias)
        permute_160: "bf16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg240_1, [1, 0]);  arg240_1 = None
        mm_127: "bf16[s1, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(empty_64, permute_160);  empty_64 = permute_160 = None

        # No stacktrace found for following nodes
        fused_add_rms_norm_default = torch.ops._C.fused_add_rms_norm.default(mm_127, embedding, arg241_1, 1e-05);  embedding = arg241_1 = fused_add_rms_norm_default = None

         # File: /home/lsakka/vllm/vllm/_custom_ops.py:639 in reshape_and_cache_flash, code: torch.ops._C_cache_ops.reshape_and_cache_flash(key, value, key_cache,
        copy_: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg13_1, arg13_1);  arg13_1 = copy_ = None
        copy__1: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg26_1, arg26_1);  arg26_1 = copy__1 = None
        copy__2: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg33_1, arg33_1);  arg33_1 = copy__2 = None
        copy__3: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg40_1, arg40_1);  arg40_1 = copy__3 = None
        copy__4: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg47_1, arg47_1);  arg47_1 = copy__4 = None
        copy__5: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg54_1, arg54_1);  arg54_1 = copy__5 = None
        copy__6: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg61_1, arg61_1);  arg61_1 = copy__6 = None
        copy__7: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg68_1, arg68_1);  arg68_1 = copy__7 = None
        copy__8: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg75_1, arg75_1);  arg75_1 = copy__8 = None
        copy__9: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg82_1, arg82_1);  arg82_1 = copy__9 = None
        copy__10: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg89_1, arg89_1);  arg89_1 = copy__10 = None
        copy__11: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg96_1, arg96_1);  arg96_1 = copy__11 = None
        copy__12: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg103_1, arg103_1);  arg103_1 = copy__12 = None
        copy__13: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg110_1, arg110_1);  arg110_1 = copy__13 = None
        copy__14: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg117_1, arg117_1);  arg117_1 = copy__14 = None
        copy__15: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg124_1, arg124_1);  arg124_1 = copy__15 = None
        copy__16: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg131_1, arg131_1);  arg131_1 = copy__16 = None
        copy__17: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg138_1, arg138_1);  arg138_1 = copy__17 = None
        copy__18: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg145_1, arg145_1);  arg145_1 = copy__18 = None
        copy__19: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg152_1, arg152_1);  arg152_1 = copy__19 = None
        copy__20: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg159_1, arg159_1);  arg159_1 = copy__20 = None
        copy__21: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg166_1, arg166_1);  arg166_1 = copy__21 = None
        copy__22: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg173_1, arg173_1);  arg173_1 = copy__22 = None
        copy__23: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg180_1, arg180_1);  arg180_1 = copy__23 = None
        copy__24: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg187_1, arg187_1);  arg187_1 = copy__24 = None
        copy__25: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg194_1, arg194_1);  arg194_1 = copy__25 = None
        copy__26: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg201_1, arg201_1);  arg201_1 = copy__26 = None
        copy__27: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg208_1, arg208_1);  arg208_1 = copy__27 = None
        copy__28: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg215_1, arg215_1);  arg215_1 = copy__28 = None
        copy__29: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg222_1, arg222_1);  arg222_1 = copy__29 = None
        copy__30: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg229_1, arg229_1);  arg229_1 = copy__30 = None
        copy__31: "bf16[s5, s6, s7, s8, s9][s6*s7*s8*s9, s7*s8*s9, s8*s9, s9, 1]cuda:0" = torch.ops.aten.copy_.default(arg236_1, arg236_1);  arg236_1 = copy__31 = None
        return (mm_127, arg5_1)

